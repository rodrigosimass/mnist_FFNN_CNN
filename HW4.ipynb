{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0457db52d070431bb605d1e12204b28e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4a1b44cbba4342478794bc5556eb9363",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0b966ebe53934f52bedc3359c941de21",
              "IPY_MODEL_93b744d7fee04baf92e68038a63778d0"
            ]
          }
        },
        "4a1b44cbba4342478794bc5556eb9363": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0b966ebe53934f52bedc3359c941de21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_39782ef9caa04735827d676928c89299",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8af493f8860541df83cad8ff62864fa0"
          }
        },
        "93b744d7fee04baf92e68038a63778d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9ecb6e46e8b74037be8425e22314d9d2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [00:08&lt;00:00,  2.15s/ file]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f80e421a58224889a0baf8e4981d1363"
          }
        },
        "39782ef9caa04735827d676928c89299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8af493f8860541df83cad8ff62864fa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ecb6e46e8b74037be8425e22314d9d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f80e421a58224889a0baf8e4981d1363": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodrigosimass/mnist_FFNN_CNN/blob/master/HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62pkex9pOy3O",
        "colab_type": "text"
      },
      "source": [
        "# New Section\n",
        "Set up\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9i6IstnU5QN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import sklearn.decomposition\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3wQDfVuVWMQ",
        "colab_type": "code",
        "outputId": "cbbb5044-94a2-4078-b248-91b8abcafeca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "0457db52d070431bb605d1e12204b28e",
            "4a1b44cbba4342478794bc5556eb9363",
            "0b966ebe53934f52bedc3359c941de21",
            "93b744d7fee04baf92e68038a63778d0",
            "39782ef9caa04735827d676928c89299",
            "8af493f8860541df83cad8ff62864fa0",
            "9ecb6e46e8b74037be8425e22314d9d2",
            "f80e421a58224889a0baf8e4981d1363"
          ]
        }
      },
      "source": [
        "mnist_data, mnist_info = tfds.load('mnist', with_info=True)\n",
        "mnist_train_x = np.asarray([instance['image']/255 for instance in tfds.as_numpy(mnist_data['train'])])\n",
        "mnist_train_y = np.asarray([instance['label'] for instance in tfds.as_numpy(mnist_data['train'])])\n",
        "mnist_test_x = np.asarray([instance['image']/255 for instance in tfds.as_numpy(mnist_data['test'])])\n",
        "mnist_test_y = np.asarray([instance['label'] for instance in tfds.as_numpy(mnist_data['test'])])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset mnist/3.0.0 (download: 11.06 MiB, generated: Unknown size, total: 11.06 MiB) to /root/tensorflow_datasets/mnist/3.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead set\n",
            "data_dir=gs://tfds-data/datasets.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0457db52d070431bb605d1e12204b28e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Dl Completed...', max=4.0, style=ProgressStyle(descriptioâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MtmlFVUL4nE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def isBest(result, results):\n",
        "  if(len(results)==0):\n",
        "    return True\n",
        "  if(result>max(results)):\n",
        "    return True\n",
        "  return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqv3bFgJzZMU",
        "colab_type": "text"
      },
      "source": [
        "Manual separation into train and validation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_jM05eLnLfW",
        "colab_type": "text"
      },
      "source": [
        "# Baseline\n",
        "No hidden "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVubGmS6ju1f",
        "colab_type": "code",
        "outputId": "2b849806-a370-45ff-9461-6c6753a15807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "mnist_baseline_model = tf.keras.Sequential(name='mnist_baseline')\n",
        "mnist_baseline_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n",
        "mnist_baseline_model.add(tf.keras.layers.Flatten(name='flatten'))\n",
        "mnist_baseline_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n",
        "mnist_baseline_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "mnist_baseline_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"mnist_baseline\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                7850      \n",
            "=================================================================\n",
            "Total params: 7,850\n",
            "Trainable params: 7,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fs-nmfRonGU",
        "colab_type": "code",
        "outputId": "34531c34-3aaf-4515-fa53-3ddbd1e16b04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_baseline_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "\n",
        "mnist_baseline_model_train = mnist_baseline_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 1.6875 - accuracy: 0.5556\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.75017, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 1.6875 - accuracy: 0.5556 - val_loss: 1.2538 - val_accuracy: 0.7502\n",
            "Epoch 2/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 1.0729 - accuracy: 0.7803\n",
            "Epoch 00002: val_accuracy improved from 0.75017 to 0.80417, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 1.0635 - accuracy: 0.7819 - val_loss: 0.9255 - val_accuracy: 0.8042\n",
            "Epoch 3/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.8461 - accuracy: 0.8170\n",
            "Epoch 00003: val_accuracy improved from 0.80417 to 0.82792, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.8420 - accuracy: 0.8181 - val_loss: 0.7782 - val_accuracy: 0.8279\n",
            "Epoch 4/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.7311 - accuracy: 0.8356\n",
            "Epoch 00004: val_accuracy improved from 0.82792 to 0.84108, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.7300 - accuracy: 0.8358 - val_loss: 0.6937 - val_accuracy: 0.8411\n",
            "Epoch 5/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.6622 - accuracy: 0.8459\n",
            "Epoch 00005: val_accuracy improved from 0.84108 to 0.85142, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.6611 - accuracy: 0.8464 - val_loss: 0.6381 - val_accuracy: 0.8514\n",
            "Epoch 6/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.6141 - accuracy: 0.8536\n",
            "Epoch 00006: val_accuracy improved from 0.85142 to 0.85617, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.6138 - accuracy: 0.8534 - val_loss: 0.5983 - val_accuracy: 0.8562\n",
            "Epoch 7/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.5800 - accuracy: 0.8588\n",
            "Epoch 00007: val_accuracy improved from 0.85617 to 0.86042, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.5791 - accuracy: 0.8591 - val_loss: 0.5684 - val_accuracy: 0.8604\n",
            "Epoch 8/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5529 - accuracy: 0.8624\n",
            "Epoch 00008: val_accuracy improved from 0.86042 to 0.86425, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.5522 - accuracy: 0.8627 - val_loss: 0.5448 - val_accuracy: 0.8643\n",
            "Epoch 9/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.5304 - accuracy: 0.8671\n",
            "Epoch 00009: val_accuracy improved from 0.86425 to 0.86875, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.5307 - accuracy: 0.8668 - val_loss: 0.5254 - val_accuracy: 0.8687\n",
            "Epoch 10/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.5131 - accuracy: 0.8693\n",
            "Epoch 00010: val_accuracy improved from 0.86875 to 0.87200, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.5130 - accuracy: 0.8695 - val_loss: 0.5094 - val_accuracy: 0.8720\n",
            "Epoch 11/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.4980 - accuracy: 0.8728\n",
            "Epoch 00011: val_accuracy improved from 0.87200 to 0.87358, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4981 - accuracy: 0.8730 - val_loss: 0.4959 - val_accuracy: 0.8736\n",
            "Epoch 12/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.4857 - accuracy: 0.8747\n",
            "Epoch 00012: val_accuracy improved from 0.87358 to 0.87583, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4854 - accuracy: 0.8748 - val_loss: 0.4842 - val_accuracy: 0.8758\n",
            "Epoch 13/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.4750 - accuracy: 0.8773\n",
            "Epoch 00013: val_accuracy improved from 0.87583 to 0.87750, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4743 - accuracy: 0.8774 - val_loss: 0.4741 - val_accuracy: 0.8775\n",
            "Epoch 14/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.4658 - accuracy: 0.8790\n",
            "Epoch 00014: val_accuracy improved from 0.87750 to 0.87925, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4646 - accuracy: 0.8790 - val_loss: 0.4651 - val_accuracy: 0.8792\n",
            "Epoch 15/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.4562 - accuracy: 0.8810\n",
            "Epoch 00015: val_accuracy improved from 0.87925 to 0.88108, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4560 - accuracy: 0.8808 - val_loss: 0.4571 - val_accuracy: 0.8811\n",
            "Epoch 16/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.4486 - accuracy: 0.8823\n",
            "Epoch 00016: val_accuracy improved from 0.88108 to 0.88267, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4483 - accuracy: 0.8821 - val_loss: 0.4500 - val_accuracy: 0.8827\n",
            "Epoch 17/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.4416 - accuracy: 0.8827\n",
            "Epoch 00017: val_accuracy improved from 0.88267 to 0.88375, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4414 - accuracy: 0.8827 - val_loss: 0.4434 - val_accuracy: 0.8838\n",
            "Epoch 18/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.4370 - accuracy: 0.8838\n",
            "Epoch 00018: val_accuracy improved from 0.88375 to 0.88483, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4351 - accuracy: 0.8845 - val_loss: 0.4376 - val_accuracy: 0.8848\n",
            "Epoch 19/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.4304 - accuracy: 0.8853\n",
            "Epoch 00019: val_accuracy improved from 0.88483 to 0.88675, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4293 - accuracy: 0.8854 - val_loss: 0.4322 - val_accuracy: 0.8867\n",
            "Epoch 20/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.4249 - accuracy: 0.8860\n",
            "Epoch 00020: val_accuracy improved from 0.88675 to 0.88708, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4241 - accuracy: 0.8862 - val_loss: 0.4272 - val_accuracy: 0.8871\n",
            "Epoch 21/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.4210 - accuracy: 0.8874\n",
            "Epoch 00021: val_accuracy improved from 0.88708 to 0.88783, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4192 - accuracy: 0.8877 - val_loss: 0.4227 - val_accuracy: 0.8878\n",
            "Epoch 22/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.4158 - accuracy: 0.8880\n",
            "Epoch 00022: val_accuracy improved from 0.88783 to 0.88808, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4147 - accuracy: 0.8885 - val_loss: 0.4185 - val_accuracy: 0.8881\n",
            "Epoch 23/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.4097 - accuracy: 0.8900\n",
            "Epoch 00023: val_accuracy improved from 0.88808 to 0.88950, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4106 - accuracy: 0.8895 - val_loss: 0.4146 - val_accuracy: 0.8895\n",
            "Epoch 24/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.4073 - accuracy: 0.8895\n",
            "Epoch 00024: val_accuracy improved from 0.88950 to 0.89033, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4066 - accuracy: 0.8897 - val_loss: 0.4108 - val_accuracy: 0.8903\n",
            "Epoch 25/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.4031 - accuracy: 0.8909\n",
            "Epoch 00025: val_accuracy improved from 0.89033 to 0.89142, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4030 - accuracy: 0.8908 - val_loss: 0.4074 - val_accuracy: 0.8914\n",
            "Epoch 26/10000\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.3996 - accuracy: 0.8918\n",
            "Epoch 00026: val_accuracy improved from 0.89142 to 0.89225, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3995 - accuracy: 0.8916 - val_loss: 0.4042 - val_accuracy: 0.8923\n",
            "Epoch 27/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3955 - accuracy: 0.8925\n",
            "Epoch 00027: val_accuracy improved from 0.89225 to 0.89325, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3963 - accuracy: 0.8922 - val_loss: 0.4012 - val_accuracy: 0.8932\n",
            "Epoch 28/10000\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.3933 - accuracy: 0.8929\n",
            "Epoch 00028: val_accuracy improved from 0.89325 to 0.89333, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3933 - accuracy: 0.8928 - val_loss: 0.3983 - val_accuracy: 0.8933\n",
            "Epoch 29/10000\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.3904 - accuracy: 0.8939\n",
            "Epoch 00029: val_accuracy improved from 0.89333 to 0.89392, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3904 - accuracy: 0.8939 - val_loss: 0.3957 - val_accuracy: 0.8939\n",
            "Epoch 30/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3881 - accuracy: 0.8946\n",
            "Epoch 00030: val_accuracy improved from 0.89392 to 0.89433, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3877 - accuracy: 0.8944 - val_loss: 0.3932 - val_accuracy: 0.8943\n",
            "Epoch 31/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3865 - accuracy: 0.8947\n",
            "Epoch 00031: val_accuracy did not improve from 0.89433\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3851 - accuracy: 0.8953 - val_loss: 0.3908 - val_accuracy: 0.8942\n",
            "Epoch 32/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3828 - accuracy: 0.8956\n",
            "Epoch 00032: val_accuracy improved from 0.89433 to 0.89467, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3826 - accuracy: 0.8958 - val_loss: 0.3884 - val_accuracy: 0.8947\n",
            "Epoch 33/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.3825 - accuracy: 0.8954\n",
            "Epoch 00033: val_accuracy improved from 0.89467 to 0.89508, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.3803 - accuracy: 0.8959 - val_loss: 0.3863 - val_accuracy: 0.8951\n",
            "Epoch 34/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3783 - accuracy: 0.8962\n",
            "Epoch 00034: val_accuracy improved from 0.89508 to 0.89542, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.3780 - accuracy: 0.8965 - val_loss: 0.3842 - val_accuracy: 0.8954\n",
            "Epoch 35/10000\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.3756 - accuracy: 0.8973\n",
            "Epoch 00035: val_accuracy improved from 0.89542 to 0.89550, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3759 - accuracy: 0.8971 - val_loss: 0.3822 - val_accuracy: 0.8955\n",
            "Epoch 36/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3748 - accuracy: 0.8974\n",
            "Epoch 00036: val_accuracy improved from 0.89550 to 0.89608, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3738 - accuracy: 0.8976 - val_loss: 0.3802 - val_accuracy: 0.8961\n",
            "Epoch 37/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.3710 - accuracy: 0.8980\n",
            "Epoch 00037: val_accuracy improved from 0.89608 to 0.89625, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3719 - accuracy: 0.8977 - val_loss: 0.3784 - val_accuracy: 0.8963\n",
            "Epoch 38/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.3705 - accuracy: 0.8975\n",
            "Epoch 00038: val_accuracy improved from 0.89625 to 0.89675, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3700 - accuracy: 0.8981 - val_loss: 0.3767 - val_accuracy: 0.8967\n",
            "Epoch 39/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3678 - accuracy: 0.8985\n",
            "Epoch 00039: val_accuracy improved from 0.89675 to 0.89725, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3682 - accuracy: 0.8984 - val_loss: 0.3750 - val_accuracy: 0.8972\n",
            "Epoch 40/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3675 - accuracy: 0.8986\n",
            "Epoch 00040: val_accuracy improved from 0.89725 to 0.89817, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3665 - accuracy: 0.8989 - val_loss: 0.3735 - val_accuracy: 0.8982\n",
            "Epoch 41/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3649 - accuracy: 0.8993\n",
            "Epoch 00041: val_accuracy improved from 0.89817 to 0.89858, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3648 - accuracy: 0.8993 - val_loss: 0.3719 - val_accuracy: 0.8986\n",
            "Epoch 42/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3630 - accuracy: 0.9000\n",
            "Epoch 00042: val_accuracy improved from 0.89858 to 0.89892, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3632 - accuracy: 0.8997 - val_loss: 0.3704 - val_accuracy: 0.8989\n",
            "Epoch 43/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.3609 - accuracy: 0.8999\n",
            "Epoch 00043: val_accuracy improved from 0.89892 to 0.89933, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3617 - accuracy: 0.8998 - val_loss: 0.3690 - val_accuracy: 0.8993\n",
            "Epoch 44/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3601 - accuracy: 0.9003\n",
            "Epoch 00044: val_accuracy improved from 0.89933 to 0.89983, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3602 - accuracy: 0.9003 - val_loss: 0.3676 - val_accuracy: 0.8998\n",
            "Epoch 45/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3592 - accuracy: 0.9007\n",
            "Epoch 00045: val_accuracy improved from 0.89983 to 0.90000, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3587 - accuracy: 0.9006 - val_loss: 0.3663 - val_accuracy: 0.9000\n",
            "Epoch 46/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.3580 - accuracy: 0.9009\n",
            "Epoch 00046: val_accuracy improved from 0.90000 to 0.90017, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3573 - accuracy: 0.9011 - val_loss: 0.3650 - val_accuracy: 0.9002\n",
            "Epoch 47/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.3543 - accuracy: 0.9019\n",
            "Epoch 00047: val_accuracy improved from 0.90017 to 0.90042, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3559 - accuracy: 0.9014 - val_loss: 0.3638 - val_accuracy: 0.9004\n",
            "Epoch 48/10000\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.3553 - accuracy: 0.9016\n",
            "Epoch 00048: val_accuracy did not improve from 0.90042\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3547 - accuracy: 0.9019 - val_loss: 0.3625 - val_accuracy: 0.9003\n",
            "Epoch 49/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3538 - accuracy: 0.9019\n",
            "Epoch 00049: val_accuracy improved from 0.90042 to 0.90108, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3534 - accuracy: 0.9019 - val_loss: 0.3614 - val_accuracy: 0.9011\n",
            "Epoch 50/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3528 - accuracy: 0.9021\n",
            "Epoch 00050: val_accuracy did not improve from 0.90108\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3521 - accuracy: 0.9024 - val_loss: 0.3603 - val_accuracy: 0.9003\n",
            "Epoch 51/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3530 - accuracy: 0.9014\n",
            "Epoch 00051: val_accuracy improved from 0.90108 to 0.90150, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3509 - accuracy: 0.9025 - val_loss: 0.3592 - val_accuracy: 0.9015\n",
            "Epoch 52/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3501 - accuracy: 0.9028\n",
            "Epoch 00052: val_accuracy improved from 0.90150 to 0.90175, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3498 - accuracy: 0.9030 - val_loss: 0.3581 - val_accuracy: 0.9018\n",
            "Epoch 53/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3479 - accuracy: 0.9032\n",
            "Epoch 00053: val_accuracy did not improve from 0.90175\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3486 - accuracy: 0.9032 - val_loss: 0.3570 - val_accuracy: 0.9016\n",
            "Epoch 54/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3474 - accuracy: 0.9034\n",
            "Epoch 00054: val_accuracy did not improve from 0.90175\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3475 - accuracy: 0.9034 - val_loss: 0.3561 - val_accuracy: 0.9018\n",
            "Epoch 55/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3466 - accuracy: 0.9040\n",
            "Epoch 00055: val_accuracy improved from 0.90175 to 0.90208, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3465 - accuracy: 0.9037 - val_loss: 0.3551 - val_accuracy: 0.9021\n",
            "Epoch 56/10000\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.3447 - accuracy: 0.9039\n",
            "Epoch 00056: val_accuracy improved from 0.90208 to 0.90283, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3454 - accuracy: 0.9039 - val_loss: 0.3542 - val_accuracy: 0.9028\n",
            "Epoch 57/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.3449 - accuracy: 0.9035\n",
            "Epoch 00057: val_accuracy did not improve from 0.90283\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3444 - accuracy: 0.9044 - val_loss: 0.3533 - val_accuracy: 0.9028\n",
            "Epoch 58/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3419 - accuracy: 0.9049\n",
            "Epoch 00058: val_accuracy did not improve from 0.90283\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3434 - accuracy: 0.9043 - val_loss: 0.3524 - val_accuracy: 0.9026\n",
            "Epoch 59/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.3419 - accuracy: 0.9051\n",
            "Epoch 00059: val_accuracy did not improve from 0.90283\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3425 - accuracy: 0.9050 - val_loss: 0.3516 - val_accuracy: 0.9028\n",
            "Epoch 60/10000\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.3415 - accuracy: 0.9050\n",
            "Epoch 00060: val_accuracy improved from 0.90283 to 0.90317, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3416 - accuracy: 0.9050 - val_loss: 0.3507 - val_accuracy: 0.9032\n",
            "Epoch 61/10000\n",
            "173/188 [==========================>...] - ETA: 0s - loss: 0.3416 - accuracy: 0.9051\n",
            "Epoch 00061: val_accuracy did not improve from 0.90317\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3406 - accuracy: 0.9054 - val_loss: 0.3499 - val_accuracy: 0.9031\n",
            "Epoch 62/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3386 - accuracy: 0.9061\n",
            "Epoch 00062: val_accuracy did not improve from 0.90317\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3398 - accuracy: 0.9053 - val_loss: 0.3491 - val_accuracy: 0.9030\n",
            "Epoch 63/10000\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.3387 - accuracy: 0.9057\n",
            "Epoch 00063: val_accuracy improved from 0.90317 to 0.90325, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3389 - accuracy: 0.9057 - val_loss: 0.3484 - val_accuracy: 0.9032\n",
            "Epoch 64/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3376 - accuracy: 0.9061\n",
            "Epoch 00064: val_accuracy improved from 0.90325 to 0.90358, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3380 - accuracy: 0.9060 - val_loss: 0.3475 - val_accuracy: 0.9036\n",
            "Epoch 65/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3366 - accuracy: 0.9063\n",
            "Epoch 00065: val_accuracy improved from 0.90358 to 0.90375, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3372 - accuracy: 0.9061 - val_loss: 0.3468 - val_accuracy: 0.9038\n",
            "Epoch 66/10000\n",
            "173/188 [==========================>...] - ETA: 0s - loss: 0.3366 - accuracy: 0.9062\n",
            "Epoch 00066: val_accuracy improved from 0.90375 to 0.90400, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3364 - accuracy: 0.9062 - val_loss: 0.3461 - val_accuracy: 0.9040\n",
            "Epoch 67/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3369 - accuracy: 0.9063\n",
            "Epoch 00067: val_accuracy improved from 0.90400 to 0.90442, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3356 - accuracy: 0.9067 - val_loss: 0.3454 - val_accuracy: 0.9044\n",
            "Epoch 68/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3342 - accuracy: 0.9065\n",
            "Epoch 00068: val_accuracy did not improve from 0.90442\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3348 - accuracy: 0.9064 - val_loss: 0.3447 - val_accuracy: 0.9043\n",
            "Epoch 69/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3336 - accuracy: 0.9073\n",
            "Epoch 00069: val_accuracy improved from 0.90442 to 0.90525, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3341 - accuracy: 0.9069 - val_loss: 0.3440 - val_accuracy: 0.9053\n",
            "Epoch 70/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3326 - accuracy: 0.9074\n",
            "Epoch 00070: val_accuracy did not improve from 0.90525\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3333 - accuracy: 0.9072 - val_loss: 0.3434 - val_accuracy: 0.9053\n",
            "Epoch 71/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3321 - accuracy: 0.9072\n",
            "Epoch 00071: val_accuracy did not improve from 0.90525\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3326 - accuracy: 0.9073 - val_loss: 0.3428 - val_accuracy: 0.9047\n",
            "Epoch 72/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3312 - accuracy: 0.9079\n",
            "Epoch 00072: val_accuracy did not improve from 0.90525\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3319 - accuracy: 0.9076 - val_loss: 0.3420 - val_accuracy: 0.9053\n",
            "Epoch 73/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.3310 - accuracy: 0.9080\n",
            "Epoch 00073: val_accuracy improved from 0.90525 to 0.90550, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3312 - accuracy: 0.9079 - val_loss: 0.3414 - val_accuracy: 0.9055\n",
            "Epoch 74/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3294 - accuracy: 0.9082\n",
            "Epoch 00074: val_accuracy improved from 0.90550 to 0.90575, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3305 - accuracy: 0.9079 - val_loss: 0.3409 - val_accuracy: 0.9057\n",
            "Epoch 75/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.3300 - accuracy: 0.9082\n",
            "Epoch 00075: val_accuracy improved from 0.90575 to 0.90633, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3298 - accuracy: 0.9082 - val_loss: 0.3402 - val_accuracy: 0.9063\n",
            "Epoch 76/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.3295 - accuracy: 0.9078\n",
            "Epoch 00076: val_accuracy did not improve from 0.90633\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3292 - accuracy: 0.9082 - val_loss: 0.3397 - val_accuracy: 0.9057\n",
            "Epoch 77/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3288 - accuracy: 0.9083\n",
            "Epoch 00077: val_accuracy did not improve from 0.90633\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3285 - accuracy: 0.9085 - val_loss: 0.3391 - val_accuracy: 0.9062\n",
            "Epoch 78/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3283 - accuracy: 0.9087\n",
            "Epoch 00078: val_accuracy did not improve from 0.90633\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3279 - accuracy: 0.9086 - val_loss: 0.3386 - val_accuracy: 0.9062\n",
            "Epoch 79/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3263 - accuracy: 0.9093\n",
            "Epoch 00079: val_accuracy improved from 0.90633 to 0.90683, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3273 - accuracy: 0.9089 - val_loss: 0.3380 - val_accuracy: 0.9068\n",
            "Epoch 80/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.3267 - accuracy: 0.9087\n",
            "Epoch 00080: val_accuracy improved from 0.90683 to 0.90708, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3267 - accuracy: 0.9088 - val_loss: 0.3375 - val_accuracy: 0.9071\n",
            "Epoch 81/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.3255 - accuracy: 0.9091\n",
            "Epoch 00081: val_accuracy improved from 0.90708 to 0.90725, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3261 - accuracy: 0.9093 - val_loss: 0.3370 - val_accuracy: 0.9072\n",
            "Epoch 82/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3251 - accuracy: 0.9092\n",
            "Epoch 00082: val_accuracy improved from 0.90725 to 0.90750, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3255 - accuracy: 0.9095 - val_loss: 0.3365 - val_accuracy: 0.9075\n",
            "Epoch 83/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3254 - accuracy: 0.9093\n",
            "Epoch 00083: val_accuracy improved from 0.90750 to 0.90758, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3249 - accuracy: 0.9094 - val_loss: 0.3360 - val_accuracy: 0.9076\n",
            "Epoch 84/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3243 - accuracy: 0.9098\n",
            "Epoch 00084: val_accuracy improved from 0.90758 to 0.90808, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3243 - accuracy: 0.9096 - val_loss: 0.3354 - val_accuracy: 0.9081\n",
            "Epoch 85/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3243 - accuracy: 0.9096\n",
            "Epoch 00085: val_accuracy did not improve from 0.90808\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3238 - accuracy: 0.9098 - val_loss: 0.3350 - val_accuracy: 0.9079\n",
            "Epoch 86/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3219 - accuracy: 0.9101\n",
            "Epoch 00086: val_accuracy improved from 0.90808 to 0.90825, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3232 - accuracy: 0.9099 - val_loss: 0.3346 - val_accuracy: 0.9082\n",
            "Epoch 87/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.3229 - accuracy: 0.9103\n",
            "Epoch 00087: val_accuracy improved from 0.90825 to 0.90850, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3227 - accuracy: 0.9100 - val_loss: 0.3340 - val_accuracy: 0.9085\n",
            "Epoch 88/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.9103\n",
            "Epoch 00088: val_accuracy did not improve from 0.90850\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3222 - accuracy: 0.9103 - val_loss: 0.3336 - val_accuracy: 0.9084\n",
            "Epoch 89/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3201 - accuracy: 0.9104\n",
            "Epoch 00089: val_accuracy did not improve from 0.90850\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3216 - accuracy: 0.9101 - val_loss: 0.3331 - val_accuracy: 0.9084\n",
            "Epoch 90/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3207 - accuracy: 0.9103\n",
            "Epoch 00090: val_accuracy improved from 0.90850 to 0.90875, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3211 - accuracy: 0.9103 - val_loss: 0.3328 - val_accuracy: 0.9087\n",
            "Epoch 91/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3201 - accuracy: 0.9107\n",
            "Epoch 00091: val_accuracy improved from 0.90875 to 0.90925, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3206 - accuracy: 0.9106 - val_loss: 0.3323 - val_accuracy: 0.9093\n",
            "Epoch 92/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3196 - accuracy: 0.9112\n",
            "Epoch 00092: val_accuracy did not improve from 0.90925\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3201 - accuracy: 0.9109 - val_loss: 0.3319 - val_accuracy: 0.9091\n",
            "Epoch 93/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3203 - accuracy: 0.9108\n",
            "Epoch 00093: val_accuracy did not improve from 0.90925\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3196 - accuracy: 0.9109 - val_loss: 0.3314 - val_accuracy: 0.9092\n",
            "Epoch 94/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3188 - accuracy: 0.9111\n",
            "Epoch 00094: val_accuracy did not improve from 0.90925\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3192 - accuracy: 0.9110 - val_loss: 0.3310 - val_accuracy: 0.9091\n",
            "Epoch 95/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.3197 - accuracy: 0.9107\n",
            "Epoch 00095: val_accuracy improved from 0.90925 to 0.90958, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3187 - accuracy: 0.9109 - val_loss: 0.3306 - val_accuracy: 0.9096\n",
            "Epoch 96/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.3190 - accuracy: 0.9108\n",
            "Epoch 00096: val_accuracy did not improve from 0.90958\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3182 - accuracy: 0.9109 - val_loss: 0.3302 - val_accuracy: 0.9096\n",
            "Epoch 97/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3156 - accuracy: 0.9118\n",
            "Epoch 00097: val_accuracy did not improve from 0.90958\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3178 - accuracy: 0.9112 - val_loss: 0.3298 - val_accuracy: 0.9095\n",
            "Epoch 98/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.3157 - accuracy: 0.9116\n",
            "Epoch 00098: val_accuracy improved from 0.90958 to 0.90983, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3173 - accuracy: 0.9112 - val_loss: 0.3294 - val_accuracy: 0.9098\n",
            "Epoch 99/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.3165 - accuracy: 0.9117\n",
            "Epoch 00099: val_accuracy did not improve from 0.90983\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3169 - accuracy: 0.9115 - val_loss: 0.3291 - val_accuracy: 0.9098\n",
            "Epoch 100/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3164 - accuracy: 0.9118\n",
            "Epoch 00100: val_accuracy did not improve from 0.90983\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3164 - accuracy: 0.9118 - val_loss: 0.3287 - val_accuracy: 0.9098\n",
            "Epoch 101/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3153 - accuracy: 0.9119\n",
            "Epoch 00101: val_accuracy improved from 0.90983 to 0.91017, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3160 - accuracy: 0.9116 - val_loss: 0.3283 - val_accuracy: 0.9102\n",
            "Epoch 102/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3165 - accuracy: 0.9114\n",
            "Epoch 00102: val_accuracy did not improve from 0.91017\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3156 - accuracy: 0.9119 - val_loss: 0.3280 - val_accuracy: 0.9101\n",
            "Epoch 103/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3159 - accuracy: 0.9120\n",
            "Epoch 00103: val_accuracy improved from 0.91017 to 0.91025, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3152 - accuracy: 0.9119 - val_loss: 0.3276 - val_accuracy: 0.9103\n",
            "Epoch 104/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3142 - accuracy: 0.9124\n",
            "Epoch 00104: val_accuracy improved from 0.91025 to 0.91033, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3147 - accuracy: 0.9122 - val_loss: 0.3273 - val_accuracy: 0.9103\n",
            "Epoch 105/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.3130 - accuracy: 0.9129\n",
            "Epoch 00105: val_accuracy improved from 0.91033 to 0.91042, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3143 - accuracy: 0.9124 - val_loss: 0.3269 - val_accuracy: 0.9104\n",
            "Epoch 106/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.3149 - accuracy: 0.9119\n",
            "Epoch 00106: val_accuracy did not improve from 0.91042\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3139 - accuracy: 0.9124 - val_loss: 0.3266 - val_accuracy: 0.9103\n",
            "Epoch 107/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3133 - accuracy: 0.9123\n",
            "Epoch 00107: val_accuracy improved from 0.91042 to 0.91083, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3135 - accuracy: 0.9125 - val_loss: 0.3263 - val_accuracy: 0.9108\n",
            "Epoch 108/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3128 - accuracy: 0.9123\n",
            "Epoch 00108: val_accuracy did not improve from 0.91083\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3132 - accuracy: 0.9124 - val_loss: 0.3259 - val_accuracy: 0.9104\n",
            "Epoch 109/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.3131 - accuracy: 0.9126\n",
            "Epoch 00109: val_accuracy did not improve from 0.91083\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3128 - accuracy: 0.9128 - val_loss: 0.3256 - val_accuracy: 0.9108\n",
            "Epoch 110/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3123 - accuracy: 0.9127\n",
            "Epoch 00110: val_accuracy improved from 0.91083 to 0.91108, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3124 - accuracy: 0.9129 - val_loss: 0.3252 - val_accuracy: 0.9111\n",
            "Epoch 111/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3118 - accuracy: 0.9132\n",
            "Epoch 00111: val_accuracy did not improve from 0.91108\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3120 - accuracy: 0.9130 - val_loss: 0.3250 - val_accuracy: 0.9107\n",
            "Epoch 112/10000\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.3123 - accuracy: 0.9127\n",
            "Epoch 00112: val_accuracy did not improve from 0.91108\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3116 - accuracy: 0.9130 - val_loss: 0.3246 - val_accuracy: 0.9107\n",
            "Epoch 113/10000\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.3105 - accuracy: 0.9132\n",
            "Epoch 00113: val_accuracy improved from 0.91108 to 0.91117, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3113 - accuracy: 0.9131 - val_loss: 0.3244 - val_accuracy: 0.9112\n",
            "Epoch 114/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3115 - accuracy: 0.9135\n",
            "Epoch 00114: val_accuracy improved from 0.91117 to 0.91125, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3109 - accuracy: 0.9133 - val_loss: 0.3240 - val_accuracy: 0.9112\n",
            "Epoch 115/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3105 - accuracy: 0.9132\n",
            "Epoch 00115: val_accuracy improved from 0.91125 to 0.91167, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3105 - accuracy: 0.9132 - val_loss: 0.3237 - val_accuracy: 0.9117\n",
            "Epoch 116/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3100 - accuracy: 0.9134\n",
            "Epoch 00116: val_accuracy improved from 0.91167 to 0.91175, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3102 - accuracy: 0.9135 - val_loss: 0.3234 - val_accuracy: 0.9118\n",
            "Epoch 117/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3100 - accuracy: 0.9136\n",
            "Epoch 00117: val_accuracy improved from 0.91175 to 0.91200, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3098 - accuracy: 0.9135 - val_loss: 0.3231 - val_accuracy: 0.9120\n",
            "Epoch 118/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.3091 - accuracy: 0.9135\n",
            "Epoch 00118: val_accuracy improved from 0.91200 to 0.91208, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3095 - accuracy: 0.9136 - val_loss: 0.3229 - val_accuracy: 0.9121\n",
            "Epoch 119/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3085 - accuracy: 0.9140\n",
            "Epoch 00119: val_accuracy improved from 0.91208 to 0.91217, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3092 - accuracy: 0.9137 - val_loss: 0.3227 - val_accuracy: 0.9122\n",
            "Epoch 120/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.3094 - accuracy: 0.9137\n",
            "Epoch 00120: val_accuracy improved from 0.91217 to 0.91225, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3088 - accuracy: 0.9139 - val_loss: 0.3223 - val_accuracy: 0.9122\n",
            "Epoch 121/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.3073 - accuracy: 0.9141\n",
            "Epoch 00121: val_accuracy improved from 0.91225 to 0.91242, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3085 - accuracy: 0.9137 - val_loss: 0.3221 - val_accuracy: 0.9124\n",
            "Epoch 122/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3075 - accuracy: 0.9144\n",
            "Epoch 00122: val_accuracy did not improve from 0.91242\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3082 - accuracy: 0.9140 - val_loss: 0.3218 - val_accuracy: 0.9124\n",
            "Epoch 123/10000\n",
            "173/188 [==========================>...] - ETA: 0s - loss: 0.3072 - accuracy: 0.9135\n",
            "Epoch 00123: val_accuracy improved from 0.91242 to 0.91250, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3078 - accuracy: 0.9141 - val_loss: 0.3215 - val_accuracy: 0.9125\n",
            "Epoch 124/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3079 - accuracy: 0.9142\n",
            "Epoch 00124: val_accuracy did not improve from 0.91250\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3075 - accuracy: 0.9142 - val_loss: 0.3213 - val_accuracy: 0.9123\n",
            "Epoch 125/10000\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.3071 - accuracy: 0.9146\n",
            "Epoch 00125: val_accuracy did not improve from 0.91250\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3072 - accuracy: 0.9144 - val_loss: 0.3210 - val_accuracy: 0.9118\n",
            "Epoch 126/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.3082 - accuracy: 0.9139\n",
            "Epoch 00126: val_accuracy improved from 0.91250 to 0.91258, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3069 - accuracy: 0.9142 - val_loss: 0.3207 - val_accuracy: 0.9126\n",
            "Epoch 127/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.3055 - accuracy: 0.9145\n",
            "Epoch 00127: val_accuracy improved from 0.91258 to 0.91267, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3066 - accuracy: 0.9144 - val_loss: 0.3205 - val_accuracy: 0.9127\n",
            "Epoch 128/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3068 - accuracy: 0.9142\n",
            "Epoch 00128: val_accuracy did not improve from 0.91267\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3063 - accuracy: 0.9144 - val_loss: 0.3202 - val_accuracy: 0.9127\n",
            "Epoch 129/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.9147\n",
            "Epoch 00129: val_accuracy improved from 0.91267 to 0.91292, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3060 - accuracy: 0.9150 - val_loss: 0.3201 - val_accuracy: 0.9129\n",
            "Epoch 130/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.3059 - accuracy: 0.9146\n",
            "Epoch 00130: val_accuracy improved from 0.91292 to 0.91333, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3057 - accuracy: 0.9147 - val_loss: 0.3198 - val_accuracy: 0.9133\n",
            "Epoch 131/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3038 - accuracy: 0.9157\n",
            "Epoch 00131: val_accuracy did not improve from 0.91333\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3054 - accuracy: 0.9148 - val_loss: 0.3195 - val_accuracy: 0.9130\n",
            "Epoch 132/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3051 - accuracy: 0.9149\n",
            "Epoch 00132: val_accuracy improved from 0.91333 to 0.91358, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3051 - accuracy: 0.9149 - val_loss: 0.3192 - val_accuracy: 0.9136\n",
            "Epoch 133/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3056 - accuracy: 0.9149\n",
            "Epoch 00133: val_accuracy did not improve from 0.91358\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3048 - accuracy: 0.9152 - val_loss: 0.3190 - val_accuracy: 0.9133\n",
            "Epoch 134/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3044 - accuracy: 0.9151\n",
            "Epoch 00134: val_accuracy did not improve from 0.91358\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3045 - accuracy: 0.9151 - val_loss: 0.3188 - val_accuracy: 0.9133\n",
            "Epoch 135/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3031 - accuracy: 0.9155\n",
            "Epoch 00135: val_accuracy improved from 0.91358 to 0.91375, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3042 - accuracy: 0.9152 - val_loss: 0.3186 - val_accuracy: 0.9137\n",
            "Epoch 136/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.3037 - accuracy: 0.9153\n",
            "Epoch 00136: val_accuracy did not improve from 0.91375\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3039 - accuracy: 0.9153 - val_loss: 0.3183 - val_accuracy: 0.9137\n",
            "Epoch 137/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3037 - accuracy: 0.9153\n",
            "Epoch 00137: val_accuracy did not improve from 0.91375\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3037 - accuracy: 0.9153 - val_loss: 0.3181 - val_accuracy: 0.9137\n",
            "Epoch 138/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.3020 - accuracy: 0.9159\n",
            "Epoch 00138: val_accuracy did not improve from 0.91375\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.3034 - accuracy: 0.9156 - val_loss: 0.3179 - val_accuracy: 0.9134\n",
            "Epoch 139/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3036 - accuracy: 0.9152\n",
            "Epoch 00139: val_accuracy did not improve from 0.91375\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3031 - accuracy: 0.9154 - val_loss: 0.3177 - val_accuracy: 0.9136\n",
            "Epoch 140/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3034 - accuracy: 0.9152\n",
            "Epoch 00140: val_accuracy did not improve from 0.91375\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3028 - accuracy: 0.9155 - val_loss: 0.3174 - val_accuracy: 0.9137\n",
            "Epoch 141/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.9156\n",
            "Epoch 00141: val_accuracy improved from 0.91375 to 0.91383, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3026 - accuracy: 0.9155 - val_loss: 0.3172 - val_accuracy: 0.9138\n",
            "Epoch 142/10000\n",
            "173/188 [==========================>...] - ETA: 0s - loss: 0.3016 - accuracy: 0.9162\n",
            "Epoch 00142: val_accuracy did not improve from 0.91383\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3023 - accuracy: 0.9156 - val_loss: 0.3170 - val_accuracy: 0.9137\n",
            "Epoch 143/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3011 - accuracy: 0.9156\n",
            "Epoch 00143: val_accuracy did not improve from 0.91383\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3021 - accuracy: 0.9156 - val_loss: 0.3168 - val_accuracy: 0.9137\n",
            "Epoch 144/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3026 - accuracy: 0.9155\n",
            "Epoch 00144: val_accuracy did not improve from 0.91383\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3018 - accuracy: 0.9156 - val_loss: 0.3166 - val_accuracy: 0.9135\n",
            "Epoch 145/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.9158\n",
            "Epoch 00145: val_accuracy did not improve from 0.91383\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3016 - accuracy: 0.9158 - val_loss: 0.3165 - val_accuracy: 0.9137\n",
            "Epoch 146/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3029 - accuracy: 0.9155\n",
            "Epoch 00146: val_accuracy did not improve from 0.91383\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3013 - accuracy: 0.9158 - val_loss: 0.3162 - val_accuracy: 0.9138\n",
            "Epoch 147/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.3018 - accuracy: 0.9160\n",
            "Epoch 00147: val_accuracy improved from 0.91383 to 0.91392, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3010 - accuracy: 0.9159 - val_loss: 0.3160 - val_accuracy: 0.9139\n",
            "Epoch 148/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.2987 - accuracy: 0.9166\n",
            "Epoch 00148: val_accuracy did not improve from 0.91392\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3008 - accuracy: 0.9159 - val_loss: 0.3158 - val_accuracy: 0.9138\n",
            "Epoch 149/10000\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.9159\n",
            "Epoch 00149: val_accuracy did not improve from 0.91392\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3005 - accuracy: 0.9161 - val_loss: 0.3156 - val_accuracy: 0.9137\n",
            "Epoch 150/10000\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.2999 - accuracy: 0.9160\n",
            "Epoch 00150: val_accuracy improved from 0.91392 to 0.91400, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3003 - accuracy: 0.9160 - val_loss: 0.3154 - val_accuracy: 0.9140\n",
            "Epoch 151/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.3007 - accuracy: 0.9160\n",
            "Epoch 00151: val_accuracy improved from 0.91400 to 0.91417, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3001 - accuracy: 0.9160 - val_loss: 0.3152 - val_accuracy: 0.9142\n",
            "Epoch 152/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.3000 - accuracy: 0.9162\n",
            "Epoch 00152: val_accuracy improved from 0.91417 to 0.91425, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2998 - accuracy: 0.9161 - val_loss: 0.3150 - val_accuracy: 0.9143\n",
            "Epoch 153/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2999 - accuracy: 0.9162\n",
            "Epoch 00153: val_accuracy did not improve from 0.91425\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2996 - accuracy: 0.9163 - val_loss: 0.3149 - val_accuracy: 0.9139\n",
            "Epoch 154/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.2991 - accuracy: 0.9164\n",
            "Epoch 00154: val_accuracy did not improve from 0.91425\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2994 - accuracy: 0.9163 - val_loss: 0.3147 - val_accuracy: 0.9141\n",
            "Epoch 155/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2985 - accuracy: 0.9165\n",
            "Epoch 00155: val_accuracy improved from 0.91425 to 0.91450, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2991 - accuracy: 0.9162 - val_loss: 0.3145 - val_accuracy: 0.9145\n",
            "Epoch 156/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.2985 - accuracy: 0.9166\n",
            "Epoch 00156: val_accuracy did not improve from 0.91450\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2989 - accuracy: 0.9164 - val_loss: 0.3144 - val_accuracy: 0.9144\n",
            "Epoch 157/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.2961 - accuracy: 0.9170\n",
            "Epoch 00157: val_accuracy did not improve from 0.91450\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2987 - accuracy: 0.9165 - val_loss: 0.3142 - val_accuracy: 0.9143\n",
            "Epoch 158/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2998 - accuracy: 0.9159\n",
            "Epoch 00158: val_accuracy improved from 0.91450 to 0.91492, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2984 - accuracy: 0.9165 - val_loss: 0.3139 - val_accuracy: 0.9149\n",
            "Epoch 159/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.2980 - accuracy: 0.9166\n",
            "Epoch 00159: val_accuracy improved from 0.91492 to 0.91508, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2982 - accuracy: 0.9166 - val_loss: 0.3137 - val_accuracy: 0.9151\n",
            "Epoch 160/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.2970 - accuracy: 0.9169\n",
            "Epoch 00160: val_accuracy did not improve from 0.91508\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2980 - accuracy: 0.9168 - val_loss: 0.3136 - val_accuracy: 0.9146\n",
            "Epoch 161/10000\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.2980 - accuracy: 0.9165\n",
            "Epoch 00161: val_accuracy did not improve from 0.91508\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2978 - accuracy: 0.9166 - val_loss: 0.3135 - val_accuracy: 0.9148\n",
            "Epoch 162/10000\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.2978 - accuracy: 0.9165\n",
            "Epoch 00162: val_accuracy improved from 0.91508 to 0.91533, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2975 - accuracy: 0.9165 - val_loss: 0.3133 - val_accuracy: 0.9153\n",
            "Epoch 163/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2964 - accuracy: 0.9174\n",
            "Epoch 00163: val_accuracy did not improve from 0.91533\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2974 - accuracy: 0.9171 - val_loss: 0.3132 - val_accuracy: 0.9150\n",
            "Epoch 164/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.2979 - accuracy: 0.9168\n",
            "Epoch 00164: val_accuracy did not improve from 0.91533\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2971 - accuracy: 0.9170 - val_loss: 0.3129 - val_accuracy: 0.9153\n",
            "Epoch 165/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.9166\n",
            "Epoch 00165: val_accuracy improved from 0.91533 to 0.91550, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2969 - accuracy: 0.9168 - val_loss: 0.3128 - val_accuracy: 0.9155\n",
            "Epoch 166/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.9172\n",
            "Epoch 00166: val_accuracy did not improve from 0.91550\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2967 - accuracy: 0.9171 - val_loss: 0.3126 - val_accuracy: 0.9151\n",
            "Epoch 167/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.2949 - accuracy: 0.9175\n",
            "Epoch 00167: val_accuracy improved from 0.91550 to 0.91575, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2965 - accuracy: 0.9173 - val_loss: 0.3124 - val_accuracy: 0.9158\n",
            "Epoch 168/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.2977 - accuracy: 0.9169\n",
            "Epoch 00168: val_accuracy did not improve from 0.91575\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2963 - accuracy: 0.9171 - val_loss: 0.3122 - val_accuracy: 0.9155\n",
            "Epoch 169/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2970 - accuracy: 0.9169\n",
            "Epoch 00169: val_accuracy did not improve from 0.91575\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2961 - accuracy: 0.9172 - val_loss: 0.3121 - val_accuracy: 0.9154\n",
            "Epoch 170/10000\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.9176\n",
            "Epoch 00170: val_accuracy did not improve from 0.91575\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2959 - accuracy: 0.9174 - val_loss: 0.3119 - val_accuracy: 0.9154\n",
            "Epoch 171/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2959 - accuracy: 0.9176\n",
            "Epoch 00171: val_accuracy did not improve from 0.91575\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2957 - accuracy: 0.9174 - val_loss: 0.3118 - val_accuracy: 0.9156\n",
            "Epoch 172/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.2959 - accuracy: 0.9172\n",
            "Epoch 00172: val_accuracy did not improve from 0.91575\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2955 - accuracy: 0.9175 - val_loss: 0.3116 - val_accuracy: 0.9157\n",
            "Epoch 173/10000\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.9173\n",
            "Epoch 00173: val_accuracy improved from 0.91575 to 0.91600, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2953 - accuracy: 0.9173 - val_loss: 0.3114 - val_accuracy: 0.9160\n",
            "Epoch 174/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.2955 - accuracy: 0.9176\n",
            "Epoch 00174: val_accuracy did not improve from 0.91600\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2951 - accuracy: 0.9178 - val_loss: 0.3114 - val_accuracy: 0.9157\n",
            "Epoch 175/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.2940 - accuracy: 0.9182\n",
            "Epoch 00175: val_accuracy did not improve from 0.91600\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2949 - accuracy: 0.9177 - val_loss: 0.3112 - val_accuracy: 0.9158\n",
            "Epoch 176/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.2929 - accuracy: 0.9180\n",
            "Epoch 00176: val_accuracy did not improve from 0.91600\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2947 - accuracy: 0.9177 - val_loss: 0.3111 - val_accuracy: 0.9159\n",
            "Epoch 177/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2952 - accuracy: 0.9178\n",
            "Epoch 00177: val_accuracy did not improve from 0.91600\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2945 - accuracy: 0.9178 - val_loss: 0.3109 - val_accuracy: 0.9158\n",
            "Epoch 178/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.2934 - accuracy: 0.9180\n",
            "Epoch 00178: val_accuracy did not improve from 0.91600\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2943 - accuracy: 0.9178 - val_loss: 0.3108 - val_accuracy: 0.9159\n",
            "Epoch 179/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.2946 - accuracy: 0.9179\n",
            "Epoch 00179: val_accuracy improved from 0.91600 to 0.91608, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2941 - accuracy: 0.9180 - val_loss: 0.3107 - val_accuracy: 0.9161\n",
            "Epoch 180/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.2971 - accuracy: 0.9171\n",
            "Epoch 00180: val_accuracy improved from 0.91608 to 0.91625, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2939 - accuracy: 0.9180 - val_loss: 0.3106 - val_accuracy: 0.9162\n",
            "Epoch 181/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.2941 - accuracy: 0.9178\n",
            "Epoch 00181: val_accuracy did not improve from 0.91625\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2937 - accuracy: 0.9179 - val_loss: 0.3103 - val_accuracy: 0.9160\n",
            "Epoch 182/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.9180\n",
            "Epoch 00182: val_accuracy did not improve from 0.91625\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2936 - accuracy: 0.9183 - val_loss: 0.3102 - val_accuracy: 0.9160\n",
            "Epoch 183/10000\n",
            "173/188 [==========================>...] - ETA: 0s - loss: 0.2939 - accuracy: 0.9176\n",
            "Epoch 00183: val_accuracy did not improve from 0.91625\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2934 - accuracy: 0.9180 - val_loss: 0.3100 - val_accuracy: 0.9161\n",
            "Epoch 184/10000\n",
            "172/188 [==========================>...] - ETA: 0s - loss: 0.2912 - accuracy: 0.9188\n",
            "Epoch 00184: val_accuracy improved from 0.91625 to 0.91633, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2932 - accuracy: 0.9181 - val_loss: 0.3099 - val_accuracy: 0.9163\n",
            "Epoch 185/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2931 - accuracy: 0.9181\n",
            "Epoch 00185: val_accuracy did not improve from 0.91633\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2930 - accuracy: 0.9182 - val_loss: 0.3098 - val_accuracy: 0.9162\n",
            "Epoch 186/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.2938 - accuracy: 0.9186\n",
            "Epoch 00186: val_accuracy did not improve from 0.91633\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2928 - accuracy: 0.9184 - val_loss: 0.3096 - val_accuracy: 0.9162\n",
            "Epoch 187/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2923 - accuracy: 0.9184\n",
            "Epoch 00187: val_accuracy did not improve from 0.91633\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2927 - accuracy: 0.9185 - val_loss: 0.3095 - val_accuracy: 0.9158\n",
            "Epoch 188/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2926 - accuracy: 0.9183\n",
            "Epoch 00188: val_accuracy did not improve from 0.91633\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2925 - accuracy: 0.9186 - val_loss: 0.3094 - val_accuracy: 0.9163\n",
            "Epoch 189/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2922 - accuracy: 0.9188\n",
            "Epoch 00189: val_accuracy improved from 0.91633 to 0.91650, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2923 - accuracy: 0.9186 - val_loss: 0.3093 - val_accuracy: 0.9165\n",
            "Epoch 190/10000\n",
            "173/188 [==========================>...] - ETA: 0s - loss: 0.2895 - accuracy: 0.9186\n",
            "Epoch 00190: val_accuracy did not improve from 0.91650\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2921 - accuracy: 0.9183 - val_loss: 0.3092 - val_accuracy: 0.9162\n",
            "Epoch 191/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2920 - accuracy: 0.9185\n",
            "Epoch 00191: val_accuracy did not improve from 0.91650\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2920 - accuracy: 0.9185 - val_loss: 0.3090 - val_accuracy: 0.9161\n",
            "Epoch 192/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.2904 - accuracy: 0.9189\n",
            "Epoch 00192: val_accuracy did not improve from 0.91650\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2918 - accuracy: 0.9187 - val_loss: 0.3089 - val_accuracy: 0.9165\n",
            "Epoch 193/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2915 - accuracy: 0.9187\n",
            "Epoch 00193: val_accuracy improved from 0.91650 to 0.91667, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2916 - accuracy: 0.9186 - val_loss: 0.3087 - val_accuracy: 0.9167\n",
            "Epoch 194/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.2929 - accuracy: 0.9183\n",
            "Epoch 00194: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2915 - accuracy: 0.9187 - val_loss: 0.3087 - val_accuracy: 0.9166\n",
            "Epoch 195/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.2910 - accuracy: 0.9187\n",
            "Epoch 00195: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2913 - accuracy: 0.9189 - val_loss: 0.3085 - val_accuracy: 0.9165\n",
            "Epoch 196/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.2920 - accuracy: 0.9186\n",
            "Epoch 00196: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2911 - accuracy: 0.9188 - val_loss: 0.3084 - val_accuracy: 0.9165\n",
            "Epoch 197/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.2903 - accuracy: 0.9193\n",
            "Epoch 00197: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2910 - accuracy: 0.9190 - val_loss: 0.3083 - val_accuracy: 0.9165\n",
            "Epoch 198/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.2912 - accuracy: 0.9187\n",
            "Epoch 00198: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2908 - accuracy: 0.9189 - val_loss: 0.3081 - val_accuracy: 0.9165\n",
            "Epoch 199/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.2905 - accuracy: 0.9186\n",
            "Epoch 00199: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2907 - accuracy: 0.9189 - val_loss: 0.3080 - val_accuracy: 0.9166\n",
            "Epoch 200/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.2898 - accuracy: 0.9192\n",
            "Epoch 00200: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2905 - accuracy: 0.9189 - val_loss: 0.3079 - val_accuracy: 0.9164\n",
            "Epoch 201/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2900 - accuracy: 0.9191\n",
            "Epoch 00201: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2903 - accuracy: 0.9190 - val_loss: 0.3078 - val_accuracy: 0.9167\n",
            "Epoch 202/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.2890 - accuracy: 0.9196\n",
            "Epoch 00202: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2902 - accuracy: 0.9190 - val_loss: 0.3076 - val_accuracy: 0.9166\n",
            "Epoch 203/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2906 - accuracy: 0.9194\n",
            "Epoch 00203: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2900 - accuracy: 0.9192 - val_loss: 0.3075 - val_accuracy: 0.9165\n",
            "Epoch 00203: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbZDKedepXS-",
        "colab_type": "code",
        "outputId": "c41485a0-dec3-4022-bd5b-934430c23f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n",
        "\n",
        "loss_ax.set_title('Loss')\n",
        "loss_ax.plot(mnist_baseline_model_train.history['loss'], '-r', label='Train')\n",
        "loss_ax.plot(mnist_baseline_model_train.history['val_loss'], '-g', label='Validation')\n",
        "\n",
        "acc_ax.set_title('Accuracy')\n",
        "acc_ax.plot(mnist_baseline_model_train.history['accuracy'], '-r', label='Train')\n",
        "acc_ax.plot(mnist_baseline_model_train.history['val_accuracy'], '-g', label='Validation')\n",
        "\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PQl3fIHN01i",
        "colab_type": "text"
      },
      "source": [
        "test the baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYJSzhKX3PYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruKJGkKZtjmS",
        "colab_type": "text"
      },
      "source": [
        "#Feed-Forward Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m66f-8VHLscW",
        "colab_type": "text"
      },
      "source": [
        "test several models with different number of layers and neurons\n",
        "\n",
        "cross validation is used and the best fold is selected in order to assess performance\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1U08t0Gxx4_",
        "colab_type": "code",
        "outputId": "65a2c792-c48d-4a14-fd4e-890c1597d171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        }
      },
      "source": [
        "num_hidden_layers = [1,2]\n",
        "num_hidden_neurons = [50,100]\n",
        "num_folds = 5; #cross validation\n",
        "eps = 10000 #epochs\n",
        "\n",
        "for layers in num_hidden_layers:\n",
        "    print(\"\\n\\nhidden_layers=\",layers)\n",
        "    for num_neur in num_hidden_neurons:\n",
        "        kf = KFold(n_splits=num_folds)\n",
        "        results = []\n",
        "        i = 1\n",
        "        print(\"    num_neurons:\",num_neur)\n",
        "        for train_idx, val_idx in kf.split(mnist_train_x, mnist_train_y):\n",
        "            #SPLIT THE DATA\n",
        "            train_x = mnist_train_x[train_idx]\n",
        "            train_y = mnist_train_y[train_idx]\n",
        "            val_x = mnist_train_x[val_idx]\n",
        "            val_y = mnist_train_y[val_idx]\n",
        "            val_set = (val_x, val_y)\n",
        "\n",
        "            #BUILD THE NN\n",
        "            mlp = tf.keras.Sequential(name='MLP-1HU')\n",
        "            mlp.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n",
        "            mlp.add(tf.keras.layers.Flatten(name='flatten'))\n",
        "            for i in range (layers):\n",
        "                nm = \"HU\" + str(i)\n",
        "                mlp.add(tf.keras.layers.Dense(num_neur, activation='tanh', name=nm))\n",
        "            mlp.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n",
        "\n",
        "            mlp.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "            #DEFINE CALL BACKS\n",
        "            earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=0)\n",
        "            checkpoint_train = tf.keras.callbacks.ModelCheckpoint('MLP-1HU.h5', monitor='accuracy', verbose=0, save_best_only=True)\n",
        "            checkpoint_valid = tf.keras.callbacks.ModelCheckpoint('multi_layer_2.h5', monitor='val_accuracy', verbose=0, save_best_only=True)\n",
        "\n",
        "            #TRAIN\n",
        "            mlp_train = mlp.fit(mnist_train_x, mnist_train_y, validation_data= val_set, callbacks=[earlystop,checkpoint_train,checkpoint_valid], epochs=eps, batch_size=256, verbose=0)\n",
        "\n",
        "            #ASSESS PERFORMANCE\n",
        "            mlp.load_weights('multi_layer_2.h5')\n",
        "            if(isBest(checkpoint_valid.best,results)):\n",
        "                best_val = checkpoint_valid.best\n",
        "                best_train = checkpoint_train.best\n",
        "            results.append(checkpoint_valid.best);\n",
        "            \n",
        "        print(results)\n",
        "        print(\"        val:\",best_val,\" trn:\",best_train)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "hidden_layers= 3\n",
            "    num_neurons: 50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-3196bfc8de24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m#TRAIN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mmlp_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_train_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearlystop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheckpoint_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheckpoint_valid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m#ASSESS PERFORMANCE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    870\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m    873\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1079\u001b[0m                 step_num=step):\n\u001b[1;32m   1080\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zQWNfTsMXt2",
        "colab_type": "text"
      },
      "source": [
        "train one model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgjTRRpp9JDu",
        "colab_type": "code",
        "outputId": "1cf1b1c5-2350-4993-9173-20503c39784d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "num_hidden_layers = 2\n",
        "num_neur = 100\n",
        "num_folds = 5; #cross validation\n",
        "eps = 10000 #epochs\n",
        "\n",
        "kf = KFold(n_splits=num_folds)\n",
        "results = []\n",
        "i = 1\n",
        "for train_idx, val_idx in kf.split(mnist_train_x, mnist_train_y):\n",
        "    print(\"fold\",i,\"/\",num_folds)\n",
        "    i+=1\n",
        "    #SPLIT THE DATA\n",
        "    train_x = mnist_train_x[train_idx]\n",
        "    train_y = mnist_train_y[train_idx]\n",
        "    val_x = mnist_train_x[val_idx]\n",
        "    val_y = mnist_train_y[val_idx]\n",
        "    val_set = (val_x, val_y)\n",
        "\n",
        "    #BUILD THE NN\n",
        "    mlp = tf.keras.Sequential(name='MLP-1HU')\n",
        "    mlp.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n",
        "    mlp.add(tf.keras.layers.Flatten(name='flatten'))\n",
        "    for j in range (num_hidden_layers):\n",
        "        nm = \"HU\" + str(j)\n",
        "        mlp.add(tf.keras.layers.Dense(num_neur, activation='tanh', name=nm))\n",
        "    mlp.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n",
        "\n",
        "    mlp.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "    #DEFINE CALL BACKS\n",
        "    earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=0)\n",
        "    checkpoint_train = tf.keras.callbacks.ModelCheckpoint('MLP-1HU.h5', monitor='accuracy', verbose=0, save_best_only=True)\n",
        "    checkpoint_valid = tf.keras.callbacks.ModelCheckpoint('multi_layer_2.h5', monitor='val_accuracy', verbose=0, save_best_only=True)\n",
        "\n",
        "    #TRAIN\n",
        "    mlp_train = mlp.fit(mnist_train_x, mnist_train_y, validation_data= val_set, callbacks=[earlystop,checkpoint_train,checkpoint_valid], epochs=eps, batch_size=256, verbose=0)\n",
        "\n",
        "    #ASSESS PERFORMANCE\n",
        "    mlp.load_weights('multi_layer_2.h5')\n",
        "    if(isBest(checkpoint_valid.best,results)): #TODO verificar se esta a usar a melhor validation\n",
        "        #SAVE MODEL OF BEST FOLD\n",
        "        mlp.save_weights('best_MLP.h5')\n",
        "        best_model = mlp_train\n",
        "        best_val = checkpoint_valid.best\n",
        "        best_train = checkpoint_train.best\n",
        "    results.append(checkpoint_valid.best);"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold 1 / 5\n",
            "fold 2 / 5\n",
            "fold 3 / 5\n",
            "fold 4 / 5\n",
            "fold 5 / 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2v3Z8bHSHyK",
        "colab_type": "text"
      },
      "source": [
        "plot the train:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FieZ2bS6mro",
        "colab_type": "code",
        "outputId": "6a93d0f6-13f3-4d6e-c2f9-7ee223807124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n",
        "\n",
        "loss_ax.set_title('Loss')\n",
        "loss_ax.plot(best_model.history['loss'], '-r', label='Train')\n",
        "loss_ax.plot(best_model.history['val_loss'], '-g', label='Validation')\n",
        "acc_ax.set_title('Accuracy')\n",
        "acc_ax.plot(best_model.history['accuracy'], '-r', label='Train')\n",
        "acc_ax.plot(best_model.history['val_accuracy'], '-g', label='Validation')\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TcdhIlESC-i",
        "colab_type": "text"
      },
      "source": [
        "and finally... use the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8pE1P1L6j89",
        "colab_type": "code",
        "outputId": "b1254e52-d89a-43c2-ef60-e7b0b7dd1248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "#stats of all the folds\n",
        "print(\"All folds:\")\n",
        "print(results)\n",
        "#satts of best fold\n",
        "print(\"Best fold:\")\n",
        "print(\"valid acc:\",best_val)\n",
        "print(\"train acc:\",best_train)\n",
        "#and finally we test\n",
        "mlp.load_weights('best_MLP.h5')\n",
        "loss, acc = mlp.evaluate(mnist_test_x, mnist_test_y)\n",
        "print('\\n---------------------------------------\\n| TEST ACCURACY =  {}  |\\n---------------------------------------\\n'.format(acc))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All folds:\n",
            "[0.9929999709129333, 0.9897500276565552, 0.996999979019165, 0.9950000047683716, 0.9981666803359985]\n",
            "Best fold:\n",
            "valid acc: 0.9981666803359985\n",
            "train acc: 0.9980666637420654\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0727 - accuracy: 0.9778\n",
            "\n",
            "---------------------------------------\n",
            "| TEST ACCURACY =  0.9778000116348267  |\n",
            "---------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qCmljaEXETt",
        "colab_type": "text"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBj7FyNJPB7l",
        "colab_type": "code",
        "outputId": "31712be5-7291-477e-fc78-a9ed7cd6e807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tf.random.set_seed(1) #to ensure same data split for all models\n",
        "\n",
        "conv_model = tf.keras.Sequential(name='mnist_cnn')\n",
        "conv_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n",
        "conv_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution'))\n",
        "conv_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling'))\n",
        "conv_model.add(tf.keras.layers.Flatten(name='flatten'))\n",
        "conv_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n",
        "conv_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "conv_earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n",
        "conv_checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_conv_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "\n",
        "mnist_conv_model_train = conv_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2,\n",
        "                                        callbacks=[conv_earlystop,conv_checkpoint], \n",
        "                                        epochs=10000, batch_size=256)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" tf.random.set_seed(1) #to ensure same data split for all models\\n\\nconv_model = tf.keras.Sequential(name='mnist_cnn')\\nconv_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\\nconv_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution'))\\nconv_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling'))\\nconv_model.add(tf.keras.layers.Flatten(name='flatten'))\\nconv_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\\nconv_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\\n\\nconv_earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\\nconv_checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_conv_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\\n\\nmnist_conv_model_train = conv_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2,\\n                                        callbacks=[conv_earlystop,conv_checkpoint], \\n                                        epochs=10000, batch_size=256) \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azHtcTJLXTVL",
        "colab_type": "code",
        "outputId": "b87f7b06-909a-453f-9810-05cb62e3471c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n",
        "\n",
        "loss_ax.set_title('Loss')\n",
        "loss_ax.plot(mnist_conv_model_train.history['loss'], '-r', label='Train')\n",
        "loss_ax.plot(mnist_conv_model_train.history['val_loss'], '-g', label='Validation')\n",
        "\n",
        "acc_ax.set_title('Accuracy')\n",
        "acc_ax.plot(mnist_conv_model_train.history['accuracy'], '-r', label='Train')\n",
        "acc_ax.plot(mnist_conv_model_train.history['val_accuracy'], '-g', label='Validation')\n",
        "\n",
        "plt.legend(loc=4)\n",
        "plt.show() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\\n\\nloss_ax.set_title('Loss')\\nloss_ax.plot(mnist_conv_model_train.history['loss'], '-r', label='Train')\\nloss_ax.plot(mnist_conv_model_train.history['val_loss'], '-g', label='Validation')\\n\\nacc_ax.set_title('Accuracy')\\nacc_ax.plot(mnist_conv_model_train.history['accuracy'], '-r', label='Train')\\nacc_ax.plot(mnist_conv_model_train.history['val_accuracy'], '-g', label='Validation')\\n\\nplt.legend(loc=4)\\nplt.show() \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}