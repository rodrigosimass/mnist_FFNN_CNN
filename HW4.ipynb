{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodrigosimass/mnist_FFNN_CNN/blob/master/HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9i6IstnU5QN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import sklearn.decomposition\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3wQDfVuVWMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_data, mnist_info = tfds.load('mnist', with_info=True)\n",
        "mnist_train_x = np.asarray([instance['image']/255 for instance in tfds.as_numpy(mnist_data['train'])])\n",
        "mnist_train_y = np.asarray([instance['label'] for instance in tfds.as_numpy(mnist_data['train'])])\n",
        "mnist_test_x = np.asarray([instance['image']/255 for instance in tfds.as_numpy(mnist_data['test'])])\n",
        "mnist_test_y = np.asarray([instance['label'] for instance in tfds.as_numpy(mnist_data['test'])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqv3bFgJzZMU",
        "colab_type": "text"
      },
      "source": [
        "Manual separation into train and validation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afqcfxt3SrTe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87fa9b36-c333-4220-a861-19fca4a428c0"
      },
      "source": [
        "\"\"\"val_x, mnist_train_x = np.split(mnist_train_x,[10000])\n",
        "val_y, mnist_train_y = np.split(mnist_train_y,[10000])\n",
        "val_set = (val_x,val_y)\"\"\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'val_x, mnist_train_x = np.split(mnist_train_x,[10000])\\nval_y, mnist_train_y = np.split(mnist_train_y,[10000])\\nval_set = (val_x,val_y)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_jM05eLnLfW",
        "colab_type": "text"
      },
      "source": [
        "# Baseline\n",
        "No hidden "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVubGmS6ju1f",
        "colab_type": "code",
        "outputId": "2b849806-a370-45ff-9461-6c6753a15807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "mnist_baseline_model = tf.keras.Sequential(name='mnist_baseline')\n",
        "mnist_baseline_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n",
        "mnist_baseline_model.add(tf.keras.layers.Flatten(name='flatten'))\n",
        "mnist_baseline_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n",
        "mnist_baseline_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "mnist_baseline_model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"mnist_baseline\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                7850      \n",
            "=================================================================\n",
            "Total params: 7,850\n",
            "Trainable params: 7,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fs-nmfRonGU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "34531c34-3aaf-4515-fa53-3ddbd1e16b04"
      },
      "source": [
        "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_baseline_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "\n",
        "mnist_baseline_model_train = mnist_baseline_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 1.6875 - accuracy: 0.5556\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.75017, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 1.6875 - accuracy: 0.5556 - val_loss: 1.2538 - val_accuracy: 0.7502\n",
            "Epoch 2/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 1.0729 - accuracy: 0.7803\n",
            "Epoch 00002: val_accuracy improved from 0.75017 to 0.80417, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 1.0635 - accuracy: 0.7819 - val_loss: 0.9255 - val_accuracy: 0.8042\n",
            "Epoch 3/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.8461 - accuracy: 0.8170\n",
            "Epoch 00003: val_accuracy improved from 0.80417 to 0.82792, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.8420 - accuracy: 0.8181 - val_loss: 0.7782 - val_accuracy: 0.8279\n",
            "Epoch 4/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.7311 - accuracy: 0.8356\n",
            "Epoch 00004: val_accuracy improved from 0.82792 to 0.84108, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.7300 - accuracy: 0.8358 - val_loss: 0.6937 - val_accuracy: 0.8411\n",
            "Epoch 5/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.6622 - accuracy: 0.8459\n",
            "Epoch 00005: val_accuracy improved from 0.84108 to 0.85142, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.6611 - accuracy: 0.8464 - val_loss: 0.6381 - val_accuracy: 0.8514\n",
            "Epoch 6/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.6141 - accuracy: 0.8536\n",
            "Epoch 00006: val_accuracy improved from 0.85142 to 0.85617, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.6138 - accuracy: 0.8534 - val_loss: 0.5983 - val_accuracy: 0.8562\n",
            "Epoch 7/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.5800 - accuracy: 0.8588\n",
            "Epoch 00007: val_accuracy improved from 0.85617 to 0.86042, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.5791 - accuracy: 0.8591 - val_loss: 0.5684 - val_accuracy: 0.8604\n",
            "Epoch 8/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5529 - accuracy: 0.8624\n",
            "Epoch 00008: val_accuracy improved from 0.86042 to 0.86425, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.5522 - accuracy: 0.8627 - val_loss: 0.5448 - val_accuracy: 0.8643\n",
            "Epoch 9/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.5304 - accuracy: 0.8671\n",
            "Epoch 00009: val_accuracy improved from 0.86425 to 0.86875, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.5307 - accuracy: 0.8668 - val_loss: 0.5254 - val_accuracy: 0.8687\n",
            "Epoch 10/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.5131 - accuracy: 0.8693\n",
            "Epoch 00010: val_accuracy improved from 0.86875 to 0.87200, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.5130 - accuracy: 0.8695 - val_loss: 0.5094 - val_accuracy: 0.8720\n",
            "Epoch 11/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.4980 - accuracy: 0.8728\n",
            "Epoch 00011: val_accuracy improved from 0.87200 to 0.87358, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4981 - accuracy: 0.8730 - val_loss: 0.4959 - val_accuracy: 0.8736\n",
            "Epoch 12/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.4857 - accuracy: 0.8747\n",
            "Epoch 00012: val_accuracy improved from 0.87358 to 0.87583, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4854 - accuracy: 0.8748 - val_loss: 0.4842 - val_accuracy: 0.8758\n",
            "Epoch 13/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.4750 - accuracy: 0.8773\n",
            "Epoch 00013: val_accuracy improved from 0.87583 to 0.87750, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4743 - accuracy: 0.8774 - val_loss: 0.4741 - val_accuracy: 0.8775\n",
            "Epoch 14/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.4658 - accuracy: 0.8790\n",
            "Epoch 00014: val_accuracy improved from 0.87750 to 0.87925, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4646 - accuracy: 0.8790 - val_loss: 0.4651 - val_accuracy: 0.8792\n",
            "Epoch 15/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.4562 - accuracy: 0.8810\n",
            "Epoch 00015: val_accuracy improved from 0.87925 to 0.88108, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4560 - accuracy: 0.8808 - val_loss: 0.4571 - val_accuracy: 0.8811\n",
            "Epoch 16/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.4486 - accuracy: 0.8823\n",
            "Epoch 00016: val_accuracy improved from 0.88108 to 0.88267, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4483 - accuracy: 0.8821 - val_loss: 0.4500 - val_accuracy: 0.8827\n",
            "Epoch 17/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.4416 - accuracy: 0.8827\n",
            "Epoch 00017: val_accuracy improved from 0.88267 to 0.88375, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4414 - accuracy: 0.8827 - val_loss: 0.4434 - val_accuracy: 0.8838\n",
            "Epoch 18/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.4370 - accuracy: 0.8838\n",
            "Epoch 00018: val_accuracy improved from 0.88375 to 0.88483, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4351 - accuracy: 0.8845 - val_loss: 0.4376 - val_accuracy: 0.8848\n",
            "Epoch 19/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.4304 - accuracy: 0.8853\n",
            "Epoch 00019: val_accuracy improved from 0.88483 to 0.88675, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4293 - accuracy: 0.8854 - val_loss: 0.4322 - val_accuracy: 0.8867\n",
            "Epoch 20/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.4249 - accuracy: 0.8860\n",
            "Epoch 00020: val_accuracy improved from 0.88675 to 0.88708, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4241 - accuracy: 0.8862 - val_loss: 0.4272 - val_accuracy: 0.8871\n",
            "Epoch 21/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.4210 - accuracy: 0.8874\n",
            "Epoch 00021: val_accuracy improved from 0.88708 to 0.88783, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4192 - accuracy: 0.8877 - val_loss: 0.4227 - val_accuracy: 0.8878\n",
            "Epoch 22/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.4158 - accuracy: 0.8880\n",
            "Epoch 00022: val_accuracy improved from 0.88783 to 0.88808, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4147 - accuracy: 0.8885 - val_loss: 0.4185 - val_accuracy: 0.8881\n",
            "Epoch 23/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.4097 - accuracy: 0.8900\n",
            "Epoch 00023: val_accuracy improved from 0.88808 to 0.88950, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4106 - accuracy: 0.8895 - val_loss: 0.4146 - val_accuracy: 0.8895\n",
            "Epoch 24/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.4073 - accuracy: 0.8895\n",
            "Epoch 00024: val_accuracy improved from 0.88950 to 0.89033, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4066 - accuracy: 0.8897 - val_loss: 0.4108 - val_accuracy: 0.8903\n",
            "Epoch 25/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.4031 - accuracy: 0.8909\n",
            "Epoch 00025: val_accuracy improved from 0.89033 to 0.89142, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4030 - accuracy: 0.8908 - val_loss: 0.4074 - val_accuracy: 0.8914\n",
            "Epoch 26/10000\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.3996 - accuracy: 0.8918\n",
            "Epoch 00026: val_accuracy improved from 0.89142 to 0.89225, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3995 - accuracy: 0.8916 - val_loss: 0.4042 - val_accuracy: 0.8923\n",
            "Epoch 27/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3955 - accuracy: 0.8925\n",
            "Epoch 00027: val_accuracy improved from 0.89225 to 0.89325, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3963 - accuracy: 0.8922 - val_loss: 0.4012 - val_accuracy: 0.8932\n",
            "Epoch 28/10000\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.3933 - accuracy: 0.8929\n",
            "Epoch 00028: val_accuracy improved from 0.89325 to 0.89333, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3933 - accuracy: 0.8928 - val_loss: 0.3983 - val_accuracy: 0.8933\n",
            "Epoch 29/10000\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.3904 - accuracy: 0.8939\n",
            "Epoch 00029: val_accuracy improved from 0.89333 to 0.89392, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3904 - accuracy: 0.8939 - val_loss: 0.3957 - val_accuracy: 0.8939\n",
            "Epoch 30/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3881 - accuracy: 0.8946\n",
            "Epoch 00030: val_accuracy improved from 0.89392 to 0.89433, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3877 - accuracy: 0.8944 - val_loss: 0.3932 - val_accuracy: 0.8943\n",
            "Epoch 31/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3865 - accuracy: 0.8947\n",
            "Epoch 00031: val_accuracy did not improve from 0.89433\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3851 - accuracy: 0.8953 - val_loss: 0.3908 - val_accuracy: 0.8942\n",
            "Epoch 32/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3828 - accuracy: 0.8956\n",
            "Epoch 00032: val_accuracy improved from 0.89433 to 0.89467, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3826 - accuracy: 0.8958 - val_loss: 0.3884 - val_accuracy: 0.8947\n",
            "Epoch 33/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.3825 - accuracy: 0.8954\n",
            "Epoch 00033: val_accuracy improved from 0.89467 to 0.89508, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.3803 - accuracy: 0.8959 - val_loss: 0.3863 - val_accuracy: 0.8951\n",
            "Epoch 34/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3783 - accuracy: 0.8962\n",
            "Epoch 00034: val_accuracy improved from 0.89508 to 0.89542, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.3780 - accuracy: 0.8965 - val_loss: 0.3842 - val_accuracy: 0.8954\n",
            "Epoch 35/10000\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.3756 - accuracy: 0.8973\n",
            "Epoch 00035: val_accuracy improved from 0.89542 to 0.89550, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3759 - accuracy: 0.8971 - val_loss: 0.3822 - val_accuracy: 0.8955\n",
            "Epoch 36/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3748 - accuracy: 0.8974\n",
            "Epoch 00036: val_accuracy improved from 0.89550 to 0.89608, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3738 - accuracy: 0.8976 - val_loss: 0.3802 - val_accuracy: 0.8961\n",
            "Epoch 37/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.3710 - accuracy: 0.8980\n",
            "Epoch 00037: val_accuracy improved from 0.89608 to 0.89625, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3719 - accuracy: 0.8977 - val_loss: 0.3784 - val_accuracy: 0.8963\n",
            "Epoch 38/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.3705 - accuracy: 0.8975\n",
            "Epoch 00038: val_accuracy improved from 0.89625 to 0.89675, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3700 - accuracy: 0.8981 - val_loss: 0.3767 - val_accuracy: 0.8967\n",
            "Epoch 39/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3678 - accuracy: 0.8985\n",
            "Epoch 00039: val_accuracy improved from 0.89675 to 0.89725, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3682 - accuracy: 0.8984 - val_loss: 0.3750 - val_accuracy: 0.8972\n",
            "Epoch 40/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3675 - accuracy: 0.8986\n",
            "Epoch 00040: val_accuracy improved from 0.89725 to 0.89817, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3665 - accuracy: 0.8989 - val_loss: 0.3735 - val_accuracy: 0.8982\n",
            "Epoch 41/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3649 - accuracy: 0.8993\n",
            "Epoch 00041: val_accuracy improved from 0.89817 to 0.89858, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3648 - accuracy: 0.8993 - val_loss: 0.3719 - val_accuracy: 0.8986\n",
            "Epoch 42/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3630 - accuracy: 0.9000\n",
            "Epoch 00042: val_accuracy improved from 0.89858 to 0.89892, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3632 - accuracy: 0.8997 - val_loss: 0.3704 - val_accuracy: 0.8989\n",
            "Epoch 43/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.3609 - accuracy: 0.8999\n",
            "Epoch 00043: val_accuracy improved from 0.89892 to 0.89933, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3617 - accuracy: 0.8998 - val_loss: 0.3690 - val_accuracy: 0.8993\n",
            "Epoch 44/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3601 - accuracy: 0.9003\n",
            "Epoch 00044: val_accuracy improved from 0.89933 to 0.89983, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3602 - accuracy: 0.9003 - val_loss: 0.3676 - val_accuracy: 0.8998\n",
            "Epoch 45/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3592 - accuracy: 0.9007\n",
            "Epoch 00045: val_accuracy improved from 0.89983 to 0.90000, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3587 - accuracy: 0.9006 - val_loss: 0.3663 - val_accuracy: 0.9000\n",
            "Epoch 46/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.3580 - accuracy: 0.9009\n",
            "Epoch 00046: val_accuracy improved from 0.90000 to 0.90017, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3573 - accuracy: 0.9011 - val_loss: 0.3650 - val_accuracy: 0.9002\n",
            "Epoch 47/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.3543 - accuracy: 0.9019\n",
            "Epoch 00047: val_accuracy improved from 0.90017 to 0.90042, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3559 - accuracy: 0.9014 - val_loss: 0.3638 - val_accuracy: 0.9004\n",
            "Epoch 48/10000\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.3553 - accuracy: 0.9016\n",
            "Epoch 00048: val_accuracy did not improve from 0.90042\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3547 - accuracy: 0.9019 - val_loss: 0.3625 - val_accuracy: 0.9003\n",
            "Epoch 49/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3538 - accuracy: 0.9019\n",
            "Epoch 00049: val_accuracy improved from 0.90042 to 0.90108, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3534 - accuracy: 0.9019 - val_loss: 0.3614 - val_accuracy: 0.9011\n",
            "Epoch 50/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3528 - accuracy: 0.9021\n",
            "Epoch 00050: val_accuracy did not improve from 0.90108\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3521 - accuracy: 0.9024 - val_loss: 0.3603 - val_accuracy: 0.9003\n",
            "Epoch 51/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3530 - accuracy: 0.9014\n",
            "Epoch 00051: val_accuracy improved from 0.90108 to 0.90150, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3509 - accuracy: 0.9025 - val_loss: 0.3592 - val_accuracy: 0.9015\n",
            "Epoch 52/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3501 - accuracy: 0.9028\n",
            "Epoch 00052: val_accuracy improved from 0.90150 to 0.90175, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3498 - accuracy: 0.9030 - val_loss: 0.3581 - val_accuracy: 0.9018\n",
            "Epoch 53/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3479 - accuracy: 0.9032\n",
            "Epoch 00053: val_accuracy did not improve from 0.90175\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3486 - accuracy: 0.9032 - val_loss: 0.3570 - val_accuracy: 0.9016\n",
            "Epoch 54/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3474 - accuracy: 0.9034\n",
            "Epoch 00054: val_accuracy did not improve from 0.90175\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3475 - accuracy: 0.9034 - val_loss: 0.3561 - val_accuracy: 0.9018\n",
            "Epoch 55/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3466 - accuracy: 0.9040\n",
            "Epoch 00055: val_accuracy improved from 0.90175 to 0.90208, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3465 - accuracy: 0.9037 - val_loss: 0.3551 - val_accuracy: 0.9021\n",
            "Epoch 56/10000\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.3447 - accuracy: 0.9039\n",
            "Epoch 00056: val_accuracy improved from 0.90208 to 0.90283, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3454 - accuracy: 0.9039 - val_loss: 0.3542 - val_accuracy: 0.9028\n",
            "Epoch 57/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.3449 - accuracy: 0.9035\n",
            "Epoch 00057: val_accuracy did not improve from 0.90283\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3444 - accuracy: 0.9044 - val_loss: 0.3533 - val_accuracy: 0.9028\n",
            "Epoch 58/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3419 - accuracy: 0.9049\n",
            "Epoch 00058: val_accuracy did not improve from 0.90283\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3434 - accuracy: 0.9043 - val_loss: 0.3524 - val_accuracy: 0.9026\n",
            "Epoch 59/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.3419 - accuracy: 0.9051\n",
            "Epoch 00059: val_accuracy did not improve from 0.90283\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3425 - accuracy: 0.9050 - val_loss: 0.3516 - val_accuracy: 0.9028\n",
            "Epoch 60/10000\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.3415 - accuracy: 0.9050\n",
            "Epoch 00060: val_accuracy improved from 0.90283 to 0.90317, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3416 - accuracy: 0.9050 - val_loss: 0.3507 - val_accuracy: 0.9032\n",
            "Epoch 61/10000\n",
            "173/188 [==========================>...] - ETA: 0s - loss: 0.3416 - accuracy: 0.9051\n",
            "Epoch 00061: val_accuracy did not improve from 0.90317\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3406 - accuracy: 0.9054 - val_loss: 0.3499 - val_accuracy: 0.9031\n",
            "Epoch 62/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3386 - accuracy: 0.9061\n",
            "Epoch 00062: val_accuracy did not improve from 0.90317\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3398 - accuracy: 0.9053 - val_loss: 0.3491 - val_accuracy: 0.9030\n",
            "Epoch 63/10000\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.3387 - accuracy: 0.9057\n",
            "Epoch 00063: val_accuracy improved from 0.90317 to 0.90325, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3389 - accuracy: 0.9057 - val_loss: 0.3484 - val_accuracy: 0.9032\n",
            "Epoch 64/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3376 - accuracy: 0.9061\n",
            "Epoch 00064: val_accuracy improved from 0.90325 to 0.90358, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3380 - accuracy: 0.9060 - val_loss: 0.3475 - val_accuracy: 0.9036\n",
            "Epoch 65/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3366 - accuracy: 0.9063\n",
            "Epoch 00065: val_accuracy improved from 0.90358 to 0.90375, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3372 - accuracy: 0.9061 - val_loss: 0.3468 - val_accuracy: 0.9038\n",
            "Epoch 66/10000\n",
            "173/188 [==========================>...] - ETA: 0s - loss: 0.3366 - accuracy: 0.9062\n",
            "Epoch 00066: val_accuracy improved from 0.90375 to 0.90400, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3364 - accuracy: 0.9062 - val_loss: 0.3461 - val_accuracy: 0.9040\n",
            "Epoch 67/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3369 - accuracy: 0.9063\n",
            "Epoch 00067: val_accuracy improved from 0.90400 to 0.90442, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3356 - accuracy: 0.9067 - val_loss: 0.3454 - val_accuracy: 0.9044\n",
            "Epoch 68/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3342 - accuracy: 0.9065\n",
            "Epoch 00068: val_accuracy did not improve from 0.90442\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3348 - accuracy: 0.9064 - val_loss: 0.3447 - val_accuracy: 0.9043\n",
            "Epoch 69/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3336 - accuracy: 0.9073\n",
            "Epoch 00069: val_accuracy improved from 0.90442 to 0.90525, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3341 - accuracy: 0.9069 - val_loss: 0.3440 - val_accuracy: 0.9053\n",
            "Epoch 70/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3326 - accuracy: 0.9074\n",
            "Epoch 00070: val_accuracy did not improve from 0.90525\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3333 - accuracy: 0.9072 - val_loss: 0.3434 - val_accuracy: 0.9053\n",
            "Epoch 71/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3321 - accuracy: 0.9072\n",
            "Epoch 00071: val_accuracy did not improve from 0.90525\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3326 - accuracy: 0.9073 - val_loss: 0.3428 - val_accuracy: 0.9047\n",
            "Epoch 72/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3312 - accuracy: 0.9079\n",
            "Epoch 00072: val_accuracy did not improve from 0.90525\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3319 - accuracy: 0.9076 - val_loss: 0.3420 - val_accuracy: 0.9053\n",
            "Epoch 73/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.3310 - accuracy: 0.9080\n",
            "Epoch 00073: val_accuracy improved from 0.90525 to 0.90550, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3312 - accuracy: 0.9079 - val_loss: 0.3414 - val_accuracy: 0.9055\n",
            "Epoch 74/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3294 - accuracy: 0.9082\n",
            "Epoch 00074: val_accuracy improved from 0.90550 to 0.90575, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3305 - accuracy: 0.9079 - val_loss: 0.3409 - val_accuracy: 0.9057\n",
            "Epoch 75/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.3300 - accuracy: 0.9082\n",
            "Epoch 00075: val_accuracy improved from 0.90575 to 0.90633, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3298 - accuracy: 0.9082 - val_loss: 0.3402 - val_accuracy: 0.9063\n",
            "Epoch 76/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.3295 - accuracy: 0.9078\n",
            "Epoch 00076: val_accuracy did not improve from 0.90633\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3292 - accuracy: 0.9082 - val_loss: 0.3397 - val_accuracy: 0.9057\n",
            "Epoch 77/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3288 - accuracy: 0.9083\n",
            "Epoch 00077: val_accuracy did not improve from 0.90633\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3285 - accuracy: 0.9085 - val_loss: 0.3391 - val_accuracy: 0.9062\n",
            "Epoch 78/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3283 - accuracy: 0.9087\n",
            "Epoch 00078: val_accuracy did not improve from 0.90633\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3279 - accuracy: 0.9086 - val_loss: 0.3386 - val_accuracy: 0.9062\n",
            "Epoch 79/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3263 - accuracy: 0.9093\n",
            "Epoch 00079: val_accuracy improved from 0.90633 to 0.90683, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3273 - accuracy: 0.9089 - val_loss: 0.3380 - val_accuracy: 0.9068\n",
            "Epoch 80/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.3267 - accuracy: 0.9087\n",
            "Epoch 00080: val_accuracy improved from 0.90683 to 0.90708, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3267 - accuracy: 0.9088 - val_loss: 0.3375 - val_accuracy: 0.9071\n",
            "Epoch 81/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.3255 - accuracy: 0.9091\n",
            "Epoch 00081: val_accuracy improved from 0.90708 to 0.90725, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3261 - accuracy: 0.9093 - val_loss: 0.3370 - val_accuracy: 0.9072\n",
            "Epoch 82/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3251 - accuracy: 0.9092\n",
            "Epoch 00082: val_accuracy improved from 0.90725 to 0.90750, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3255 - accuracy: 0.9095 - val_loss: 0.3365 - val_accuracy: 0.9075\n",
            "Epoch 83/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3254 - accuracy: 0.9093\n",
            "Epoch 00083: val_accuracy improved from 0.90750 to 0.90758, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3249 - accuracy: 0.9094 - val_loss: 0.3360 - val_accuracy: 0.9076\n",
            "Epoch 84/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3243 - accuracy: 0.9098\n",
            "Epoch 00084: val_accuracy improved from 0.90758 to 0.90808, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3243 - accuracy: 0.9096 - val_loss: 0.3354 - val_accuracy: 0.9081\n",
            "Epoch 85/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3243 - accuracy: 0.9096\n",
            "Epoch 00085: val_accuracy did not improve from 0.90808\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3238 - accuracy: 0.9098 - val_loss: 0.3350 - val_accuracy: 0.9079\n",
            "Epoch 86/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3219 - accuracy: 0.9101\n",
            "Epoch 00086: val_accuracy improved from 0.90808 to 0.90825, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3232 - accuracy: 0.9099 - val_loss: 0.3346 - val_accuracy: 0.9082\n",
            "Epoch 87/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.3229 - accuracy: 0.9103\n",
            "Epoch 00087: val_accuracy improved from 0.90825 to 0.90850, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3227 - accuracy: 0.9100 - val_loss: 0.3340 - val_accuracy: 0.9085\n",
            "Epoch 88/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.9103\n",
            "Epoch 00088: val_accuracy did not improve from 0.90850\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3222 - accuracy: 0.9103 - val_loss: 0.3336 - val_accuracy: 0.9084\n",
            "Epoch 89/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3201 - accuracy: 0.9104\n",
            "Epoch 00089: val_accuracy did not improve from 0.90850\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3216 - accuracy: 0.9101 - val_loss: 0.3331 - val_accuracy: 0.9084\n",
            "Epoch 90/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3207 - accuracy: 0.9103\n",
            "Epoch 00090: val_accuracy improved from 0.90850 to 0.90875, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3211 - accuracy: 0.9103 - val_loss: 0.3328 - val_accuracy: 0.9087\n",
            "Epoch 91/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3201 - accuracy: 0.9107\n",
            "Epoch 00091: val_accuracy improved from 0.90875 to 0.90925, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3206 - accuracy: 0.9106 - val_loss: 0.3323 - val_accuracy: 0.9093\n",
            "Epoch 92/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3196 - accuracy: 0.9112\n",
            "Epoch 00092: val_accuracy did not improve from 0.90925\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3201 - accuracy: 0.9109 - val_loss: 0.3319 - val_accuracy: 0.9091\n",
            "Epoch 93/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3203 - accuracy: 0.9108\n",
            "Epoch 00093: val_accuracy did not improve from 0.90925\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3196 - accuracy: 0.9109 - val_loss: 0.3314 - val_accuracy: 0.9092\n",
            "Epoch 94/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3188 - accuracy: 0.9111\n",
            "Epoch 00094: val_accuracy did not improve from 0.90925\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3192 - accuracy: 0.9110 - val_loss: 0.3310 - val_accuracy: 0.9091\n",
            "Epoch 95/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.3197 - accuracy: 0.9107\n",
            "Epoch 00095: val_accuracy improved from 0.90925 to 0.90958, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3187 - accuracy: 0.9109 - val_loss: 0.3306 - val_accuracy: 0.9096\n",
            "Epoch 96/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.3190 - accuracy: 0.9108\n",
            "Epoch 00096: val_accuracy did not improve from 0.90958\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3182 - accuracy: 0.9109 - val_loss: 0.3302 - val_accuracy: 0.9096\n",
            "Epoch 97/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3156 - accuracy: 0.9118\n",
            "Epoch 00097: val_accuracy did not improve from 0.90958\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3178 - accuracy: 0.9112 - val_loss: 0.3298 - val_accuracy: 0.9095\n",
            "Epoch 98/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.3157 - accuracy: 0.9116\n",
            "Epoch 00098: val_accuracy improved from 0.90958 to 0.90983, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3173 - accuracy: 0.9112 - val_loss: 0.3294 - val_accuracy: 0.9098\n",
            "Epoch 99/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.3165 - accuracy: 0.9117\n",
            "Epoch 00099: val_accuracy did not improve from 0.90983\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3169 - accuracy: 0.9115 - val_loss: 0.3291 - val_accuracy: 0.9098\n",
            "Epoch 100/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3164 - accuracy: 0.9118\n",
            "Epoch 00100: val_accuracy did not improve from 0.90983\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3164 - accuracy: 0.9118 - val_loss: 0.3287 - val_accuracy: 0.9098\n",
            "Epoch 101/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3153 - accuracy: 0.9119\n",
            "Epoch 00101: val_accuracy improved from 0.90983 to 0.91017, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3160 - accuracy: 0.9116 - val_loss: 0.3283 - val_accuracy: 0.9102\n",
            "Epoch 102/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3165 - accuracy: 0.9114\n",
            "Epoch 00102: val_accuracy did not improve from 0.91017\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3156 - accuracy: 0.9119 - val_loss: 0.3280 - val_accuracy: 0.9101\n",
            "Epoch 103/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3159 - accuracy: 0.9120\n",
            "Epoch 00103: val_accuracy improved from 0.91017 to 0.91025, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3152 - accuracy: 0.9119 - val_loss: 0.3276 - val_accuracy: 0.9103\n",
            "Epoch 104/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3142 - accuracy: 0.9124\n",
            "Epoch 00104: val_accuracy improved from 0.91025 to 0.91033, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3147 - accuracy: 0.9122 - val_loss: 0.3273 - val_accuracy: 0.9103\n",
            "Epoch 105/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.3130 - accuracy: 0.9129\n",
            "Epoch 00105: val_accuracy improved from 0.91033 to 0.91042, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3143 - accuracy: 0.9124 - val_loss: 0.3269 - val_accuracy: 0.9104\n",
            "Epoch 106/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.3149 - accuracy: 0.9119\n",
            "Epoch 00106: val_accuracy did not improve from 0.91042\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3139 - accuracy: 0.9124 - val_loss: 0.3266 - val_accuracy: 0.9103\n",
            "Epoch 107/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3133 - accuracy: 0.9123\n",
            "Epoch 00107: val_accuracy improved from 0.91042 to 0.91083, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3135 - accuracy: 0.9125 - val_loss: 0.3263 - val_accuracy: 0.9108\n",
            "Epoch 108/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3128 - accuracy: 0.9123\n",
            "Epoch 00108: val_accuracy did not improve from 0.91083\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3132 - accuracy: 0.9124 - val_loss: 0.3259 - val_accuracy: 0.9104\n",
            "Epoch 109/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.3131 - accuracy: 0.9126\n",
            "Epoch 00109: val_accuracy did not improve from 0.91083\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3128 - accuracy: 0.9128 - val_loss: 0.3256 - val_accuracy: 0.9108\n",
            "Epoch 110/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3123 - accuracy: 0.9127\n",
            "Epoch 00110: val_accuracy improved from 0.91083 to 0.91108, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3124 - accuracy: 0.9129 - val_loss: 0.3252 - val_accuracy: 0.9111\n",
            "Epoch 111/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3118 - accuracy: 0.9132\n",
            "Epoch 00111: val_accuracy did not improve from 0.91108\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3120 - accuracy: 0.9130 - val_loss: 0.3250 - val_accuracy: 0.9107\n",
            "Epoch 112/10000\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.3123 - accuracy: 0.9127\n",
            "Epoch 00112: val_accuracy did not improve from 0.91108\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3116 - accuracy: 0.9130 - val_loss: 0.3246 - val_accuracy: 0.9107\n",
            "Epoch 113/10000\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.3105 - accuracy: 0.9132\n",
            "Epoch 00113: val_accuracy improved from 0.91108 to 0.91117, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3113 - accuracy: 0.9131 - val_loss: 0.3244 - val_accuracy: 0.9112\n",
            "Epoch 114/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3115 - accuracy: 0.9135\n",
            "Epoch 00114: val_accuracy improved from 0.91117 to 0.91125, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3109 - accuracy: 0.9133 - val_loss: 0.3240 - val_accuracy: 0.9112\n",
            "Epoch 115/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3105 - accuracy: 0.9132\n",
            "Epoch 00115: val_accuracy improved from 0.91125 to 0.91167, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3105 - accuracy: 0.9132 - val_loss: 0.3237 - val_accuracy: 0.9117\n",
            "Epoch 116/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3100 - accuracy: 0.9134\n",
            "Epoch 00116: val_accuracy improved from 0.91167 to 0.91175, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3102 - accuracy: 0.9135 - val_loss: 0.3234 - val_accuracy: 0.9118\n",
            "Epoch 117/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3100 - accuracy: 0.9136\n",
            "Epoch 00117: val_accuracy improved from 0.91175 to 0.91200, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3098 - accuracy: 0.9135 - val_loss: 0.3231 - val_accuracy: 0.9120\n",
            "Epoch 118/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.3091 - accuracy: 0.9135\n",
            "Epoch 00118: val_accuracy improved from 0.91200 to 0.91208, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3095 - accuracy: 0.9136 - val_loss: 0.3229 - val_accuracy: 0.9121\n",
            "Epoch 119/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3085 - accuracy: 0.9140\n",
            "Epoch 00119: val_accuracy improved from 0.91208 to 0.91217, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3092 - accuracy: 0.9137 - val_loss: 0.3227 - val_accuracy: 0.9122\n",
            "Epoch 120/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.3094 - accuracy: 0.9137\n",
            "Epoch 00120: val_accuracy improved from 0.91217 to 0.91225, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3088 - accuracy: 0.9139 - val_loss: 0.3223 - val_accuracy: 0.9122\n",
            "Epoch 121/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.3073 - accuracy: 0.9141\n",
            "Epoch 00121: val_accuracy improved from 0.91225 to 0.91242, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3085 - accuracy: 0.9137 - val_loss: 0.3221 - val_accuracy: 0.9124\n",
            "Epoch 122/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3075 - accuracy: 0.9144\n",
            "Epoch 00122: val_accuracy did not improve from 0.91242\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3082 - accuracy: 0.9140 - val_loss: 0.3218 - val_accuracy: 0.9124\n",
            "Epoch 123/10000\n",
            "173/188 [==========================>...] - ETA: 0s - loss: 0.3072 - accuracy: 0.9135\n",
            "Epoch 00123: val_accuracy improved from 0.91242 to 0.91250, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3078 - accuracy: 0.9141 - val_loss: 0.3215 - val_accuracy: 0.9125\n",
            "Epoch 124/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3079 - accuracy: 0.9142\n",
            "Epoch 00124: val_accuracy did not improve from 0.91250\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3075 - accuracy: 0.9142 - val_loss: 0.3213 - val_accuracy: 0.9123\n",
            "Epoch 125/10000\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.3071 - accuracy: 0.9146\n",
            "Epoch 00125: val_accuracy did not improve from 0.91250\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3072 - accuracy: 0.9144 - val_loss: 0.3210 - val_accuracy: 0.9118\n",
            "Epoch 126/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.3082 - accuracy: 0.9139\n",
            "Epoch 00126: val_accuracy improved from 0.91250 to 0.91258, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3069 - accuracy: 0.9142 - val_loss: 0.3207 - val_accuracy: 0.9126\n",
            "Epoch 127/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.3055 - accuracy: 0.9145\n",
            "Epoch 00127: val_accuracy improved from 0.91258 to 0.91267, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3066 - accuracy: 0.9144 - val_loss: 0.3205 - val_accuracy: 0.9127\n",
            "Epoch 128/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3068 - accuracy: 0.9142\n",
            "Epoch 00128: val_accuracy did not improve from 0.91267\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3063 - accuracy: 0.9144 - val_loss: 0.3202 - val_accuracy: 0.9127\n",
            "Epoch 129/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.9147\n",
            "Epoch 00129: val_accuracy improved from 0.91267 to 0.91292, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3060 - accuracy: 0.9150 - val_loss: 0.3201 - val_accuracy: 0.9129\n",
            "Epoch 130/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.3059 - accuracy: 0.9146\n",
            "Epoch 00130: val_accuracy improved from 0.91292 to 0.91333, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3057 - accuracy: 0.9147 - val_loss: 0.3198 - val_accuracy: 0.9133\n",
            "Epoch 131/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3038 - accuracy: 0.9157\n",
            "Epoch 00131: val_accuracy did not improve from 0.91333\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3054 - accuracy: 0.9148 - val_loss: 0.3195 - val_accuracy: 0.9130\n",
            "Epoch 132/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3051 - accuracy: 0.9149\n",
            "Epoch 00132: val_accuracy improved from 0.91333 to 0.91358, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3051 - accuracy: 0.9149 - val_loss: 0.3192 - val_accuracy: 0.9136\n",
            "Epoch 133/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3056 - accuracy: 0.9149\n",
            "Epoch 00133: val_accuracy did not improve from 0.91358\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3048 - accuracy: 0.9152 - val_loss: 0.3190 - val_accuracy: 0.9133\n",
            "Epoch 134/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3044 - accuracy: 0.9151\n",
            "Epoch 00134: val_accuracy did not improve from 0.91358\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3045 - accuracy: 0.9151 - val_loss: 0.3188 - val_accuracy: 0.9133\n",
            "Epoch 135/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3031 - accuracy: 0.9155\n",
            "Epoch 00135: val_accuracy improved from 0.91358 to 0.91375, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3042 - accuracy: 0.9152 - val_loss: 0.3186 - val_accuracy: 0.9137\n",
            "Epoch 136/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.3037 - accuracy: 0.9153\n",
            "Epoch 00136: val_accuracy did not improve from 0.91375\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3039 - accuracy: 0.9153 - val_loss: 0.3183 - val_accuracy: 0.9137\n",
            "Epoch 137/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3037 - accuracy: 0.9153\n",
            "Epoch 00137: val_accuracy did not improve from 0.91375\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3037 - accuracy: 0.9153 - val_loss: 0.3181 - val_accuracy: 0.9137\n",
            "Epoch 138/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.3020 - accuracy: 0.9159\n",
            "Epoch 00138: val_accuracy did not improve from 0.91375\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.3034 - accuracy: 0.9156 - val_loss: 0.3179 - val_accuracy: 0.9134\n",
            "Epoch 139/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3036 - accuracy: 0.9152\n",
            "Epoch 00139: val_accuracy did not improve from 0.91375\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3031 - accuracy: 0.9154 - val_loss: 0.3177 - val_accuracy: 0.9136\n",
            "Epoch 140/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3034 - accuracy: 0.9152\n",
            "Epoch 00140: val_accuracy did not improve from 0.91375\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3028 - accuracy: 0.9155 - val_loss: 0.3174 - val_accuracy: 0.9137\n",
            "Epoch 141/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.9156\n",
            "Epoch 00141: val_accuracy improved from 0.91375 to 0.91383, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3026 - accuracy: 0.9155 - val_loss: 0.3172 - val_accuracy: 0.9138\n",
            "Epoch 142/10000\n",
            "173/188 [==========================>...] - ETA: 0s - loss: 0.3016 - accuracy: 0.9162\n",
            "Epoch 00142: val_accuracy did not improve from 0.91383\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3023 - accuracy: 0.9156 - val_loss: 0.3170 - val_accuracy: 0.9137\n",
            "Epoch 143/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3011 - accuracy: 0.9156\n",
            "Epoch 00143: val_accuracy did not improve from 0.91383\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3021 - accuracy: 0.9156 - val_loss: 0.3168 - val_accuracy: 0.9137\n",
            "Epoch 144/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3026 - accuracy: 0.9155\n",
            "Epoch 00144: val_accuracy did not improve from 0.91383\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3018 - accuracy: 0.9156 - val_loss: 0.3166 - val_accuracy: 0.9135\n",
            "Epoch 145/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.9158\n",
            "Epoch 00145: val_accuracy did not improve from 0.91383\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3016 - accuracy: 0.9158 - val_loss: 0.3165 - val_accuracy: 0.9137\n",
            "Epoch 146/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3029 - accuracy: 0.9155\n",
            "Epoch 00146: val_accuracy did not improve from 0.91383\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3013 - accuracy: 0.9158 - val_loss: 0.3162 - val_accuracy: 0.9138\n",
            "Epoch 147/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.3018 - accuracy: 0.9160\n",
            "Epoch 00147: val_accuracy improved from 0.91383 to 0.91392, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3010 - accuracy: 0.9159 - val_loss: 0.3160 - val_accuracy: 0.9139\n",
            "Epoch 148/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.2987 - accuracy: 0.9166\n",
            "Epoch 00148: val_accuracy did not improve from 0.91392\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3008 - accuracy: 0.9159 - val_loss: 0.3158 - val_accuracy: 0.9138\n",
            "Epoch 149/10000\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.9159\n",
            "Epoch 00149: val_accuracy did not improve from 0.91392\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3005 - accuracy: 0.9161 - val_loss: 0.3156 - val_accuracy: 0.9137\n",
            "Epoch 150/10000\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.2999 - accuracy: 0.9160\n",
            "Epoch 00150: val_accuracy improved from 0.91392 to 0.91400, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3003 - accuracy: 0.9160 - val_loss: 0.3154 - val_accuracy: 0.9140\n",
            "Epoch 151/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.3007 - accuracy: 0.9160\n",
            "Epoch 00151: val_accuracy improved from 0.91400 to 0.91417, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3001 - accuracy: 0.9160 - val_loss: 0.3152 - val_accuracy: 0.9142\n",
            "Epoch 152/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.3000 - accuracy: 0.9162\n",
            "Epoch 00152: val_accuracy improved from 0.91417 to 0.91425, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2998 - accuracy: 0.9161 - val_loss: 0.3150 - val_accuracy: 0.9143\n",
            "Epoch 153/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2999 - accuracy: 0.9162\n",
            "Epoch 00153: val_accuracy did not improve from 0.91425\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2996 - accuracy: 0.9163 - val_loss: 0.3149 - val_accuracy: 0.9139\n",
            "Epoch 154/10000\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.2991 - accuracy: 0.9164\n",
            "Epoch 00154: val_accuracy did not improve from 0.91425\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2994 - accuracy: 0.9163 - val_loss: 0.3147 - val_accuracy: 0.9141\n",
            "Epoch 155/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2985 - accuracy: 0.9165\n",
            "Epoch 00155: val_accuracy improved from 0.91425 to 0.91450, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2991 - accuracy: 0.9162 - val_loss: 0.3145 - val_accuracy: 0.9145\n",
            "Epoch 156/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.2985 - accuracy: 0.9166\n",
            "Epoch 00156: val_accuracy did not improve from 0.91450\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2989 - accuracy: 0.9164 - val_loss: 0.3144 - val_accuracy: 0.9144\n",
            "Epoch 157/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.2961 - accuracy: 0.9170\n",
            "Epoch 00157: val_accuracy did not improve from 0.91450\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2987 - accuracy: 0.9165 - val_loss: 0.3142 - val_accuracy: 0.9143\n",
            "Epoch 158/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2998 - accuracy: 0.9159\n",
            "Epoch 00158: val_accuracy improved from 0.91450 to 0.91492, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2984 - accuracy: 0.9165 - val_loss: 0.3139 - val_accuracy: 0.9149\n",
            "Epoch 159/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.2980 - accuracy: 0.9166\n",
            "Epoch 00159: val_accuracy improved from 0.91492 to 0.91508, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2982 - accuracy: 0.9166 - val_loss: 0.3137 - val_accuracy: 0.9151\n",
            "Epoch 160/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.2970 - accuracy: 0.9169\n",
            "Epoch 00160: val_accuracy did not improve from 0.91508\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2980 - accuracy: 0.9168 - val_loss: 0.3136 - val_accuracy: 0.9146\n",
            "Epoch 161/10000\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.2980 - accuracy: 0.9165\n",
            "Epoch 00161: val_accuracy did not improve from 0.91508\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2978 - accuracy: 0.9166 - val_loss: 0.3135 - val_accuracy: 0.9148\n",
            "Epoch 162/10000\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.2978 - accuracy: 0.9165\n",
            "Epoch 00162: val_accuracy improved from 0.91508 to 0.91533, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2975 - accuracy: 0.9165 - val_loss: 0.3133 - val_accuracy: 0.9153\n",
            "Epoch 163/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2964 - accuracy: 0.9174\n",
            "Epoch 00163: val_accuracy did not improve from 0.91533\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2974 - accuracy: 0.9171 - val_loss: 0.3132 - val_accuracy: 0.9150\n",
            "Epoch 164/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.2979 - accuracy: 0.9168\n",
            "Epoch 00164: val_accuracy did not improve from 0.91533\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2971 - accuracy: 0.9170 - val_loss: 0.3129 - val_accuracy: 0.9153\n",
            "Epoch 165/10000\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.9166\n",
            "Epoch 00165: val_accuracy improved from 0.91533 to 0.91550, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2969 - accuracy: 0.9168 - val_loss: 0.3128 - val_accuracy: 0.9155\n",
            "Epoch 166/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.9172\n",
            "Epoch 00166: val_accuracy did not improve from 0.91550\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2967 - accuracy: 0.9171 - val_loss: 0.3126 - val_accuracy: 0.9151\n",
            "Epoch 167/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.2949 - accuracy: 0.9175\n",
            "Epoch 00167: val_accuracy improved from 0.91550 to 0.91575, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2965 - accuracy: 0.9173 - val_loss: 0.3124 - val_accuracy: 0.9158\n",
            "Epoch 168/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.2977 - accuracy: 0.9169\n",
            "Epoch 00168: val_accuracy did not improve from 0.91575\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2963 - accuracy: 0.9171 - val_loss: 0.3122 - val_accuracy: 0.9155\n",
            "Epoch 169/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2970 - accuracy: 0.9169\n",
            "Epoch 00169: val_accuracy did not improve from 0.91575\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2961 - accuracy: 0.9172 - val_loss: 0.3121 - val_accuracy: 0.9154\n",
            "Epoch 170/10000\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.9176\n",
            "Epoch 00170: val_accuracy did not improve from 0.91575\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2959 - accuracy: 0.9174 - val_loss: 0.3119 - val_accuracy: 0.9154\n",
            "Epoch 171/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2959 - accuracy: 0.9176\n",
            "Epoch 00171: val_accuracy did not improve from 0.91575\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2957 - accuracy: 0.9174 - val_loss: 0.3118 - val_accuracy: 0.9156\n",
            "Epoch 172/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.2959 - accuracy: 0.9172\n",
            "Epoch 00172: val_accuracy did not improve from 0.91575\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2955 - accuracy: 0.9175 - val_loss: 0.3116 - val_accuracy: 0.9157\n",
            "Epoch 173/10000\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.9173\n",
            "Epoch 00173: val_accuracy improved from 0.91575 to 0.91600, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2953 - accuracy: 0.9173 - val_loss: 0.3114 - val_accuracy: 0.9160\n",
            "Epoch 174/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.2955 - accuracy: 0.9176\n",
            "Epoch 00174: val_accuracy did not improve from 0.91600\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2951 - accuracy: 0.9178 - val_loss: 0.3114 - val_accuracy: 0.9157\n",
            "Epoch 175/10000\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.2940 - accuracy: 0.9182\n",
            "Epoch 00175: val_accuracy did not improve from 0.91600\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2949 - accuracy: 0.9177 - val_loss: 0.3112 - val_accuracy: 0.9158\n",
            "Epoch 176/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.2929 - accuracy: 0.9180\n",
            "Epoch 00176: val_accuracy did not improve from 0.91600\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2947 - accuracy: 0.9177 - val_loss: 0.3111 - val_accuracy: 0.9159\n",
            "Epoch 177/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2952 - accuracy: 0.9178\n",
            "Epoch 00177: val_accuracy did not improve from 0.91600\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2945 - accuracy: 0.9178 - val_loss: 0.3109 - val_accuracy: 0.9158\n",
            "Epoch 178/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.2934 - accuracy: 0.9180\n",
            "Epoch 00178: val_accuracy did not improve from 0.91600\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2943 - accuracy: 0.9178 - val_loss: 0.3108 - val_accuracy: 0.9159\n",
            "Epoch 179/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.2946 - accuracy: 0.9179\n",
            "Epoch 00179: val_accuracy improved from 0.91600 to 0.91608, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2941 - accuracy: 0.9180 - val_loss: 0.3107 - val_accuracy: 0.9161\n",
            "Epoch 180/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.2971 - accuracy: 0.9171\n",
            "Epoch 00180: val_accuracy improved from 0.91608 to 0.91625, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2939 - accuracy: 0.9180 - val_loss: 0.3106 - val_accuracy: 0.9162\n",
            "Epoch 181/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.2941 - accuracy: 0.9178\n",
            "Epoch 00181: val_accuracy did not improve from 0.91625\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2937 - accuracy: 0.9179 - val_loss: 0.3103 - val_accuracy: 0.9160\n",
            "Epoch 182/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.9180\n",
            "Epoch 00182: val_accuracy did not improve from 0.91625\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2936 - accuracy: 0.9183 - val_loss: 0.3102 - val_accuracy: 0.9160\n",
            "Epoch 183/10000\n",
            "173/188 [==========================>...] - ETA: 0s - loss: 0.2939 - accuracy: 0.9176\n",
            "Epoch 00183: val_accuracy did not improve from 0.91625\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2934 - accuracy: 0.9180 - val_loss: 0.3100 - val_accuracy: 0.9161\n",
            "Epoch 184/10000\n",
            "172/188 [==========================>...] - ETA: 0s - loss: 0.2912 - accuracy: 0.9188\n",
            "Epoch 00184: val_accuracy improved from 0.91625 to 0.91633, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2932 - accuracy: 0.9181 - val_loss: 0.3099 - val_accuracy: 0.9163\n",
            "Epoch 185/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2931 - accuracy: 0.9181\n",
            "Epoch 00185: val_accuracy did not improve from 0.91633\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2930 - accuracy: 0.9182 - val_loss: 0.3098 - val_accuracy: 0.9162\n",
            "Epoch 186/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.2938 - accuracy: 0.9186\n",
            "Epoch 00186: val_accuracy did not improve from 0.91633\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2928 - accuracy: 0.9184 - val_loss: 0.3096 - val_accuracy: 0.9162\n",
            "Epoch 187/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2923 - accuracy: 0.9184\n",
            "Epoch 00187: val_accuracy did not improve from 0.91633\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2927 - accuracy: 0.9185 - val_loss: 0.3095 - val_accuracy: 0.9158\n",
            "Epoch 188/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2926 - accuracy: 0.9183\n",
            "Epoch 00188: val_accuracy did not improve from 0.91633\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2925 - accuracy: 0.9186 - val_loss: 0.3094 - val_accuracy: 0.9163\n",
            "Epoch 189/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2922 - accuracy: 0.9188\n",
            "Epoch 00189: val_accuracy improved from 0.91633 to 0.91650, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2923 - accuracy: 0.9186 - val_loss: 0.3093 - val_accuracy: 0.9165\n",
            "Epoch 190/10000\n",
            "173/188 [==========================>...] - ETA: 0s - loss: 0.2895 - accuracy: 0.9186\n",
            "Epoch 00190: val_accuracy did not improve from 0.91650\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2921 - accuracy: 0.9183 - val_loss: 0.3092 - val_accuracy: 0.9162\n",
            "Epoch 191/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2920 - accuracy: 0.9185\n",
            "Epoch 00191: val_accuracy did not improve from 0.91650\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2920 - accuracy: 0.9185 - val_loss: 0.3090 - val_accuracy: 0.9161\n",
            "Epoch 192/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.2904 - accuracy: 0.9189\n",
            "Epoch 00192: val_accuracy did not improve from 0.91650\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2918 - accuracy: 0.9187 - val_loss: 0.3089 - val_accuracy: 0.9165\n",
            "Epoch 193/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2915 - accuracy: 0.9187\n",
            "Epoch 00193: val_accuracy improved from 0.91650 to 0.91667, saving model to mnist_baseline_best.h5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2916 - accuracy: 0.9186 - val_loss: 0.3087 - val_accuracy: 0.9167\n",
            "Epoch 194/10000\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.2929 - accuracy: 0.9183\n",
            "Epoch 00194: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2915 - accuracy: 0.9187 - val_loss: 0.3087 - val_accuracy: 0.9166\n",
            "Epoch 195/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.2910 - accuracy: 0.9187\n",
            "Epoch 00195: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2913 - accuracy: 0.9189 - val_loss: 0.3085 - val_accuracy: 0.9165\n",
            "Epoch 196/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.2920 - accuracy: 0.9186\n",
            "Epoch 00196: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2911 - accuracy: 0.9188 - val_loss: 0.3084 - val_accuracy: 0.9165\n",
            "Epoch 197/10000\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.2903 - accuracy: 0.9193\n",
            "Epoch 00197: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2910 - accuracy: 0.9190 - val_loss: 0.3083 - val_accuracy: 0.9165\n",
            "Epoch 198/10000\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.2912 - accuracy: 0.9187\n",
            "Epoch 00198: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2908 - accuracy: 0.9189 - val_loss: 0.3081 - val_accuracy: 0.9165\n",
            "Epoch 199/10000\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.2905 - accuracy: 0.9186\n",
            "Epoch 00199: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2907 - accuracy: 0.9189 - val_loss: 0.3080 - val_accuracy: 0.9166\n",
            "Epoch 200/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.2898 - accuracy: 0.9192\n",
            "Epoch 00200: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2905 - accuracy: 0.9189 - val_loss: 0.3079 - val_accuracy: 0.9164\n",
            "Epoch 201/10000\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2900 - accuracy: 0.9191\n",
            "Epoch 00201: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2903 - accuracy: 0.9190 - val_loss: 0.3078 - val_accuracy: 0.9167\n",
            "Epoch 202/10000\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.2890 - accuracy: 0.9196\n",
            "Epoch 00202: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2902 - accuracy: 0.9190 - val_loss: 0.3076 - val_accuracy: 0.9166\n",
            "Epoch 203/10000\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2906 - accuracy: 0.9194\n",
            "Epoch 00203: val_accuracy did not improve from 0.91667\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2900 - accuracy: 0.9192 - val_loss: 0.3075 - val_accuracy: 0.9165\n",
            "Epoch 00203: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbZDKedepXS-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "c41485a0-dec3-4022-bd5b-934430c23f1a"
      },
      "source": [
        "fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n",
        "\n",
        "loss_ax.set_title('Loss')\n",
        "loss_ax.plot(mnist_baseline_model_train.history['loss'], '-r', label='Train')\n",
        "loss_ax.plot(mnist_baseline_model_train.history['val_loss'], '-g', label='Validation')\n",
        "\n",
        "acc_ax.set_title('Accuracy')\n",
        "acc_ax.plot(mnist_baseline_model_train.history['accuracy'], '-r', label='Train')\n",
        "acc_ax.plot(mnist_baseline_model_train.history['val_accuracy'], '-g', label='Validation')\n",
        "\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PQl3fIHN01i",
        "colab_type": "text"
      },
      "source": [
        "Convolutional 1 hidden Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYJSzhKX3PYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def isBest(result, results):\n",
        "  if(len(results)==0):\n",
        "    return True\n",
        "  if(result>max(results)):\n",
        "    return True\n",
        "  return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruKJGkKZtjmS",
        "colab_type": "text"
      },
      "source": [
        "#MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1U08t0Gxx4_",
        "colab_type": "code",
        "outputId": "f711ec53-db3f-4206-e97d-365ebb7d9326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_hidden_layers = [1,2,3]\n",
        "num_hidden_neurons = [50,100,150,200]\n",
        "num_folds = 5; #cross validation\n",
        "eps = 10000 #epochs\n",
        "\n",
        "for layers in num_hidden_layers:\n",
        "    print(\"\\n\\nhidden_layers=\",layers)\n",
        "    for num_neur in num_hidden_neurons:\n",
        "        kf = KFold(n_splits=num_folds)\n",
        "        results = []\n",
        "        i = 1\n",
        "        print(\"    num_neurons:\",num_neur)\n",
        "        for train_idx, val_idx in kf.split(mnist_train_x, mnist_train_y):\n",
        "            #print(\"fold\",i,\"/\",num_folds)\n",
        "            i+=1\n",
        "            #SPLIT THE DATA\n",
        "            train_x = mnist_train_x[train_idx]\n",
        "            train_y = mnist_train_y[train_idx]\n",
        "            val_x = mnist_train_x[val_idx]\n",
        "            val_y = mnist_train_y[val_idx]\n",
        "            val_set = (val_x, val_y)\n",
        "\n",
        "            #BUILD THE NN\n",
        "            mlp = tf.keras.Sequential(name='MLP-1HU')\n",
        "            mlp.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n",
        "            mlp.add(tf.keras.layers.Flatten(name='flatten'))\n",
        "            for i in range (layers):\n",
        "                nm = \"HU\" + str(i)\n",
        "                #print(nm, \" unit added\")\n",
        "                mlp.add(tf.keras.layers.Dense(num_neur, activation='tanh', name=nm))\n",
        "            mlp.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n",
        "\n",
        "            mlp.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "            #DEFINE CALL BACKS\n",
        "            earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n",
        "            checkpoint_train = tf.keras.callbacks.ModelCheckpoint('MLP-1HU.h5', monitor='accuracy', verbose=0, save_best_only=True)\n",
        "            checkpoint_valid = tf.keras.callbacks.ModelCheckpoint('multi_layer_2.h5', monitor='val_accuracy', verbose=0, save_best_only=True)\n",
        "\n",
        "            #TRAIN\n",
        "            mlp_train = mlp.fit(mnist_train_x, mnist_train_y, validation_data= val_set, callbacks=[earlystop,checkpoint_train,checkpoint_valid], epochs=eps, batch_size=256, verbose=0)\n",
        "\n",
        "            #ASSESS PERFORMANCE\n",
        "            mlp.load_weights('multi_layer_2.h5')\n",
        "            if(isBest(checkpoint_valid.best,results)):\n",
        "                #SAVE MODEL OF BEST FOLD\n",
        "                #mlp.save_weights('best_MLP.h5')\n",
        "                #best_train = mlp_train\n",
        "                best_val = checkpoint_valid.best\n",
        "                best_train = checkpoint_train.best\n",
        "            results.append(checkpoint_valid.best);\n",
        "            \n",
        "        #print(results)\n",
        "        #mlp.load_weights('best_MLP.h5')\n",
        "        print(\"        val:\",best_val,\" trn:\",best_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "hidden_layers= 1\n",
            "    num_neurons: 50\n",
            "Epoch 00323: early stopping\n",
            "Epoch 00363: early stopping\n",
            "Epoch 00649: early stopping\n",
            "Epoch 00384: early stopping\n",
            "Epoch 00364: early stopping\n",
            "        val: 0.987416684627533  trn: 0.9868500232696533\n",
            "    num_neurons: 100\n",
            "Epoch 00453: early stopping\n",
            "Epoch 00564: early stopping\n",
            "Epoch 00458: early stopping\n",
            "Epoch 00516: early stopping\n",
            "Epoch 00427: early stopping\n",
            "        val: 0.9891666769981384  trn: 0.9885833263397217\n",
            "    num_neurons: 150\n",
            "Epoch 00695: early stopping\n",
            "Epoch 00538: early stopping\n",
            "Epoch 00617: early stopping\n",
            "Epoch 00659: early stopping\n",
            "Epoch 00456: early stopping\n",
            "        val: 0.9926666617393494  trn: 0.993233323097229\n",
            "    num_neurons: 200\n",
            "Epoch 00484: early stopping\n",
            "Epoch 00593: early stopping\n",
            "Epoch 00474: early stopping\n",
            "Epoch 00422: early stopping\n",
            "Epoch 00494: early stopping\n",
            "        val: 0.9919999837875366  trn: 0.9910833239555359\n",
            "\n",
            "\n",
            "hidden_layers= 2\n",
            "    num_neurons: 50\n",
            "Epoch 00265: early stopping\n",
            "Epoch 00399: early stopping\n",
            "Epoch 00392: early stopping\n",
            "Epoch 00332: early stopping\n",
            "Epoch 00442: early stopping\n",
            "        val: 0.9944999814033508  trn: 0.994183361530304\n",
            "    num_neurons: 100\n",
            "Epoch 00487: early stopping\n",
            "Epoch 00247: early stopping\n",
            "Epoch 00468: early stopping\n",
            "Epoch 00491: early stopping\n",
            "Epoch 00324: early stopping\n",
            "        val: 0.9980000257492065  trn: 0.9979166388511658\n",
            "    num_neurons: 150\n",
            "Epoch 00430: early stopping\n",
            "Epoch 00286: early stopping\n",
            "Epoch 00385: early stopping\n",
            "Epoch 00450: early stopping\n",
            "Epoch 00447: early stopping\n",
            "        val: 0.9978333115577698  trn: 0.9973499774932861\n",
            "    num_neurons: 200\n",
            "Epoch 00420: early stopping\n",
            "Epoch 00355: early stopping\n",
            "Epoch 00456: early stopping\n",
            "Epoch 00442: early stopping\n",
            "Epoch 00289: early stopping\n",
            "        val: 0.9980833530426025  trn: 0.9977166652679443\n",
            "\n",
            "\n",
            "hidden_layers= 3\n",
            "    num_neurons: 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8pE1P1L6j89",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "80406fdd-6755-4085-b4e8-0b744020b6b5"
      },
      "source": [
        "\"\"\"mlp.load_weights('best_MLP.h5')\n",
        "#loss, acc = mlp.evaluate(mnist_test_x, mnist_test_y)\n",
        "print(\"All folds:\")\n",
        "print(results)\n",
        "print(\"\\nBest Model:\")\n",
        "print(\"Accuracy:\")\n",
        "print(' -TEST      : {}'.format(acc))\n",
        "print(\" -validation:\",checkpoint_valid.best)\n",
        "print(\" -train     :\",checkpoint_train.best)\"\"\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mlp.load_weights(\\'best_MLP.h5\\')\\n#loss, acc = mlp.evaluate(mnist_test_x, mnist_test_y)\\nprint(\"All folds:\")\\nprint(results)\\nprint(\"\\nBest Model:\")\\nprint(\"Accuracy:\")\\nprint(\\' -TEST      : {}\\'.format(acc))\\nprint(\" -validation:\",checkpoint_valid.best)\\nprint(\" -train     :\",checkpoint_train.best)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FieZ2bS6mro",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "45531bce-622b-4c09-b2a1-94ae25cdca80"
      },
      "source": [
        "\"\"\"fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n",
        "\n",
        "loss_ax.set_title('Loss')\n",
        "loss_ax.plot(best_train.history['loss'], '-r', label='Train')\n",
        "loss_ax.plot(best_train.history['val_loss'], '-g', label='Validation')\n",
        "acc_ax.set_title('Accuracy')\n",
        "acc_ax.plot(best_train.history['accuracy'], '-r', label='Train')\n",
        "acc_ax.plot(best_train.history['val_accuracy'], '-g', label='Validation')\n",
        "plt.legend(loc=4)\n",
        "plt.show()\"\"\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\\n\\nloss_ax.set_title('Loss')\\nloss_ax.plot(best_train.history['loss'], '-r', label='Train')\\nloss_ax.plot(best_train.history['val_loss'], '-g', label='Validation')\\nacc_ax.set_title('Accuracy')\\nacc_ax.plot(best_train.history['accuracy'], '-r', label='Train')\\nacc_ax.plot(best_train.history['val_accuracy'], '-g', label='Validation')\\nplt.legend(loc=4)\\nplt.show()\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvl7gEEJOuZ9",
        "colab_type": "text"
      },
      "source": [
        "# MLP 2 hidden Layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw156XFRmlqm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5b24afc-1a79-41ae-c98f-99efb097bb61"
      },
      "source": [
        "name = \"HU\"\n",
        "name += str(1)\n",
        "print(name)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HU1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgjTRRpp9JDu",
        "colab_type": "code",
        "outputId": "e5273abd-1e8e-443b-db4f-e6562cb78667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "for num_neur in num_hidden_neurons:\n",
        "    kf = KFold(n_splits=num_folds)\n",
        "    results = []\n",
        "    i = 1\n",
        "    print(\"\\n-----num_neurons:\",num_neur,\"-----\")\n",
        "    for train_idx, val_idx in kf.split(mnist_train_x, mnist_train_y):\n",
        "        #print(\"fold\",i,\"/\",num_folds)\n",
        "        i+=1\n",
        "        #SPLIT THE DATA\n",
        "        train_x = mnist_train_x[train_idx]\n",
        "        train_y = mnist_train_y[train_idx]\n",
        "        val_x = mnist_train_x[val_idx]\n",
        "        val_y = mnist_train_y[val_idx]\n",
        "        val_set = (val_x, val_y)\n",
        "\n",
        "        #BUILD THE NN\n",
        "        mlp = tf.keras.Sequential(name='MLP-1HU')\n",
        "        mlp.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n",
        "        mlp.add(tf.keras.layers.Flatten(name='flatten'))\n",
        "        mlp.add(tf.keras.layers.Dense(num_neur, activation='tanh', name='HU1'))\n",
        "        mlp.add(tf.keras.layers.Dense(num_neur, activation='tanh', name='HU1'))\n",
        "        mlp.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n",
        "\n",
        "        mlp.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "        #DEFINE CALL BACKS\n",
        "        earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n",
        "        checkpoint_train = tf.keras.callbacks.ModelCheckpoint('MLP-1HU.h5', monitor='accuracy', verbose=0, save_best_only=True)\n",
        "        checkpoint_valid = tf.keras.callbacks.ModelCheckpoint('multi_layer_2.h5', monitor='val_accuracy', verbose=0, save_best_only=True)\n",
        "\n",
        "        #TRAIN\n",
        "        mlp_train = mlp.fit(mnist_train_x, mnist_train_y, validation_data= val_set, callbacks=[earlystop,checkpoint_train,checkpoint_valid], epochs=eps, batch_size=256, verbose=0)\n",
        "\n",
        "        #ASSESS PERFORMANCE\n",
        "        mlp.load_weights('multi_layer_2.h5')\n",
        "        if(isBest(checkpoint_valid.best,results)):\n",
        "            #SAVE MODEL OF BEST FOLD\n",
        "            #mlp.save_weights('best_MLP.h5')\n",
        "            #best_train = mlp_train\n",
        "            best_val = checkpoint_valid.best\n",
        "            best_train = checkpoint_train.best\n",
        "        results.append(checkpoint_valid.best);\n",
        "        \n",
        "    #print(results)\n",
        "    #mlp.load_weights('best_MLP.h5')\n",
        "    print(\"val:\",best_val,\" trn:\",best_train)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-----num_neurons: 1 -----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-eeec2e796a90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'flatten'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_neur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'HU1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_neur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'HU1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mset_inputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     nodes, nodes_by_depth, layers, _ = _map_graph_network(\n\u001b[0;32m--> 307\u001b[0;31m         self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1801\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m       raise ValueError('The name \"' + name + '\" is used ' +\n\u001b[0;32m-> 1803\u001b[0;31m                        \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' times in the model. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1804\u001b[0m                        'All layer names should be unique.')\n\u001b[1;32m   1805\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnetwork_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The name \"HU1\" is used 2 times in the model. All layer names should be unique."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcOe9dHwk6Sg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "f4d0536e-7100-4f8d-c564-d6debc035929"
      },
      "source": [
        "for num_neur in num_hidden_neurons:\n",
        "    kf = KFold(n_splits=num_folds)\n",
        "    results = []\n",
        "    i = 1\n",
        "    print(\"\\n-----num_neurons:\",num_neur,\"-----\")\n",
        "    for train_idx, val_idx in kf.split(mnist_train_x, mnist_train_y):\n",
        "        #print(\"fold\",i,\"/\",num_folds)\n",
        "        i+=1\n",
        "        #SPLIT THE DATA\n",
        "        train_x = mnist_train_x[train_idx]\n",
        "        train_y = mnist_train_y[train_idx]\n",
        "        val_x = mnist_train_x[val_idx]\n",
        "        val_y = mnist_train_y[val_idx]\n",
        "        val_set = (val_x, val_y)\n",
        "\n",
        "        #BUILD THE NN\n",
        "        mlp = tf.keras.Sequential(name='MLP-1HU')\n",
        "        mlp.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n",
        "        mlp.add(tf.keras.layers.Flatten(name='flatten'))\n",
        "        mlp.add(tf.keras.layers.Dense(num_neur, activation='tanh', name='HU1'))\n",
        "        mlp.add(tf.keras.layers.Dense(num_neur, activation='tanh', name='HU2'))\n",
        "        mlp.add(tf.keras.layers.Dense(num_neur, activation='tanh', name='HU3'))\n",
        "        mlp.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n",
        "\n",
        "        mlp.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "        #DEFINE CALL BACKS\n",
        "        earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n",
        "        checkpoint_train = tf.keras.callbacks.ModelCheckpoint('MLP-1HU.h5', monitor='accuracy', verbose=0, save_best_only=True)\n",
        "        checkpoint_valid = tf.keras.callbacks.ModelCheckpoint('multi_layer_2.h5', monitor='val_accuracy', verbose=0, save_best_only=True)\n",
        "\n",
        "        #TRAIN\n",
        "        mlp_train = mlp.fit(mnist_train_x, mnist_train_y, validation_data= val_set, callbacks=[earlystop,checkpoint_train,checkpoint_valid], epochs=eps, batch_size=256, verbose=0)\n",
        "\n",
        "        #ASSESS PERFORMANCE\n",
        "        mlp.load_weights('multi_layer_2.h5')\n",
        "        if(isBest(checkpoint_valid.best,results)):\n",
        "            #SAVE MODEL OF BEST FOLD\n",
        "            #mlp.save_weights('best_MLP.h5')\n",
        "            #best_train = mlp_train\n",
        "            best_val = checkpoint_valid.best\n",
        "            best_train = checkpoint_train.best\n",
        "        results.append(checkpoint_valid.best);\n",
        "        \n",
        "    #print(results)\n",
        "    #mlp.load_weights('best_MLP.h5')\n",
        "    print(\"val:\",best_val,\" trn:\",best_train)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-----num_neurons: 1 -----\n",
            "val: 0.20576666295528412  trn: 0.2056666612625122\n",
            "\n",
            "-----num_neurons: 2 -----\n",
            "val: 0.3002333343029022  trn: 0.28415000438690186\n",
            "\n",
            "-----num_neurons: 3 -----\n",
            "val: 0.37726667523384094  trn: 0.36861667037010193\n",
            "\n",
            "-----num_neurons: 4 -----\n",
            "val: 0.43613332509994507  trn: 0.3853333294391632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qCmljaEXETt",
        "colab_type": "text"
      },
      "source": [
        "# MLP 1 Convulutional Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBj7FyNJPB7l",
        "colab_type": "code",
        "outputId": "31712be5-7291-477e-fc78-a9ed7cd6e807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "''' tf.random.set_seed(1) #to ensure same data split for all models\n",
        "\n",
        "conv_model = tf.keras.Sequential(name='mnist_cnn')\n",
        "conv_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n",
        "conv_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution'))\n",
        "conv_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling'))\n",
        "conv_model.add(tf.keras.layers.Flatten(name='flatten'))\n",
        "conv_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n",
        "conv_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "conv_earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n",
        "conv_checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_conv_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "\n",
        "mnist_conv_model_train = conv_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2,\n",
        "                                        callbacks=[conv_earlystop,conv_checkpoint], \n",
        "                                        epochs=10000, batch_size=256) '''\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" tf.random.set_seed(1) #to ensure same data split for all models\\n\\nconv_model = tf.keras.Sequential(name='mnist_cnn')\\nconv_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\\nconv_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution'))\\nconv_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling'))\\nconv_model.add(tf.keras.layers.Flatten(name='flatten'))\\nconv_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\\nconv_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\\n\\nconv_earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\\nconv_checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_conv_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\\n\\nmnist_conv_model_train = conv_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2,\\n                                        callbacks=[conv_earlystop,conv_checkpoint], \\n                                        epochs=10000, batch_size=256) \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azHtcTJLXTVL",
        "colab_type": "code",
        "outputId": "b87f7b06-909a-453f-9810-05cb62e3471c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "''' fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n",
        "\n",
        "loss_ax.set_title('Loss')\n",
        "loss_ax.plot(mnist_conv_model_train.history['loss'], '-r', label='Train')\n",
        "loss_ax.plot(mnist_conv_model_train.history['val_loss'], '-g', label='Validation')\n",
        "\n",
        "acc_ax.set_title('Accuracy')\n",
        "acc_ax.plot(mnist_conv_model_train.history['accuracy'], '-r', label='Train')\n",
        "acc_ax.plot(mnist_conv_model_train.history['val_accuracy'], '-g', label='Validation')\n",
        "\n",
        "plt.legend(loc=4)\n",
        "plt.show() '''"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\\n\\nloss_ax.set_title('Loss')\\nloss_ax.plot(mnist_conv_model_train.history['loss'], '-r', label='Train')\\nloss_ax.plot(mnist_conv_model_train.history['val_loss'], '-g', label='Validation')\\n\\nacc_ax.set_title('Accuracy')\\nacc_ax.plot(mnist_conv_model_train.history['accuracy'], '-r', label='Train')\\nacc_ax.plot(mnist_conv_model_train.history['val_accuracy'], '-g', label='Validation')\\n\\nplt.legend(loc=4)\\nplt.show() \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}